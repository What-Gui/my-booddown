<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Mathematical Background | A Minimal Book Example</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Mathematical Background | A Minimal Book Example" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Mathematical Background | A Minimal Book Example" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="introduction.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="mathematical-background.html"><a href="mathematical-background.html"><i class="fa fa-check"></i><b>1</b> Mathematical Background</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mathematical-background.html"><a href="mathematical-background.html#a-quick-overview-of-mathematical-prerequisites"><i class="fa fa-check"></i><b>1.1</b> A quick overview of mathematical prerequisites</a></li>
<li class="chapter" data-level="1.2" data-path="mathematical-background.html"><a href="mathematical-background.html#mathematical-proofs"><i class="fa fa-check"></i><b>1.2</b> Mathematical Proofs</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="mathematical-background.html"><a href="mathematical-background.html#example-the-existence-of-infinitely-many-primes."><i class="fa fa-check"></i><b>1.2.1</b> Example: The existence of infinitely many primes.</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="mathematical-background.html"><a href="mathematical-background.html#probability-and-sample-spaces"><i class="fa fa-check"></i><b>1.3</b> Probability and Sample spaces</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="mathematical-background.html"><a href="mathematical-background.html#random-variables"><i class="fa fa-check"></i><b>1.3.1</b> Random variables</a></li>
<li class="chapter" data-level="1.3.2" data-path="mathematical-background.html"><a href="mathematical-background.html#distributions-over-strings"><i class="fa fa-check"></i><b>1.3.2</b> Distributions over strings</a></li>
<li class="chapter" data-level="1.3.3" data-path="mathematical-background.html"><a href="mathematical-background.html#more-general-sample-spaces."><i class="fa fa-check"></i><b>1.3.3</b> More general sample spaces.</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="mathematical-background.html"><a href="mathematical-background.html#correlations-and-independence"><i class="fa fa-check"></i><b>1.4</b> Correlations and independence</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="mathematical-background.html"><a href="mathematical-background.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4.1</b> Independent random variables</a></li>
<li class="chapter" data-level="1.4.2" data-path="mathematical-background.html"><a href="mathematical-background.html#collections-of-independent-random-variables."><i class="fa fa-check"></i><b>1.4.2</b> Collections of independent random variables.</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="mathematical-background.html"><a href="mathematical-background.html#concentration-and-tail-bounds"><i class="fa fa-check"></i><b>1.5</b> Concentration and tail bounds</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="mathematical-background.html"><a href="mathematical-background.html#chebyshevs-inequality"><i class="fa fa-check"></i><b>1.5.1</b> Chebyshev’s Inequality</a></li>
<li class="chapter" data-level="1.5.2" data-path="mathematical-background.html"><a href="mathematical-background.html#the-chernoff-bound"><i class="fa fa-check"></i><b>1.5.2</b> The Chernoff bound</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="mathematical-background.html"><a href="mathematical-background.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
<li class="chapter" data-level="1.7" data-path="mathematical-background.html"><a href="mathematical-background.html#exercises-1"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction.html"><a href="introduction.html#some-history"><i class="fa fa-check"></i><b>2.1</b> Some history</a></li>
<li class="chapter" data-level="2.2" data-path="introduction.html"><a href="introduction.html#defining-encryptions"><i class="fa fa-check"></i><b>2.2</b> Defining encryptions</a></li>
<li class="chapter" data-level="2.3" data-path="introduction.html"><a href="introduction.html#defining-security-of-encryption"><i class="fa fa-check"></i><b>2.3</b> Defining security of encryption</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction.html"><a href="introduction.html#generating-randomness-in-actual-cryptographic-systems"><i class="fa fa-check"></i><b>2.3.1</b> Generating randomness in actual cryptographic systems</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction.html"><a href="introduction.html#defining-the-secrecy-requirement."><i class="fa fa-check"></i><b>2.4</b> Defining the secrecy requirement.</a></li>
<li class="chapter" data-level="2.5" data-path="introduction.html"><a href="introduction.html#perfect-secrecy"><i class="fa fa-check"></i><b>2.5</b> Perfect Secrecy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction.html"><a href="introduction.html#achieving-perfect-secrecy"><i class="fa fa-check"></i><b>2.5.1</b> Achieving perfect secrecy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introduction.html"><a href="introduction.html#necessity-of-long-keys"><i class="fa fa-check"></i><b>2.6</b> Necessity of long keys</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introduction.html"><a href="introduction.html#amplifying-success-probability"><i class="fa fa-check"></i><b>2.6.1</b> Amplifying success probability</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction.html"><a href="introduction.html#bibliographical-notes"><i class="fa fa-check"></i><b>2.7</b> Bibliographical notes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="computational-security.html"><a href="computational-security.html"><i class="fa fa-check"></i><b>3</b> Computational Security</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="computational-security.html"><a href="computational-security.html#proof-by-reduction"><i class="fa fa-check"></i><b>3.0.1</b> Proof by reduction</a></li>
<li class="chapter" data-level="3.1" data-path="computational-security.html"><a href="computational-security.html#the-asymptotic-approach"><i class="fa fa-check"></i><b>3.1</b> The asymptotic approach</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="computational-security.html"><a href="computational-security.html#countoperation"><i class="fa fa-check"></i><b>3.1.1</b> Counting number of operations.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="computational-security.html"><a href="computational-security.html#our-first-conjecture"><i class="fa fa-check"></i><b>3.2</b> Our first conjecture</a></li>
<li class="chapter" data-level="3.3" data-path="computational-security.html"><a href="computational-security.html#why-care-about-the-cipher-conjecture"><i class="fa fa-check"></i><b>3.3</b> Why care about the cipher conjecture?</a></li>
<li class="chapter" data-level="3.4" data-path="computational-security.html"><a href="computational-security.html#prelude-computational-indistinguishability"><i class="fa fa-check"></i><b>3.4</b> Prelude: Computational Indistinguishability</a></li>
<li class="chapter" data-level="3.5" data-path="computational-security.html"><a href="computational-security.html#the-length-extension-theorem-or-stream-ciphers"><i class="fa fa-check"></i><b>3.5</b> The Length Extension Theorem or Stream Ciphers</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="computational-security.html"><a href="computational-security.html#appendix-the-computational-model"><i class="fa fa-check"></i><b>3.5.1</b> Appendix: The computational model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="pseudorandomness.html"><a href="pseudorandomness.html"><i class="fa fa-check"></i><b>4</b> Pseudorandomness</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="pseudorandomness.html"><a href="pseudorandomness.html#unpredictability-an-alternative-approach-for-proving-the-length-extension-theorem"><i class="fa fa-check"></i><b>4.0.1</b> Unpredictability: an alternative approach for proving the length extension theorem</a></li>
<li class="chapter" data-level="4.1" data-path="pseudorandomness.html"><a href="pseudorandomness.html#stream-ciphers"><i class="fa fa-check"></i><b>4.1</b> Stream ciphers</a></li>
<li class="chapter" data-level="4.2" data-path="pseudorandomness.html"><a href="pseudorandomness.html#what-do-pseudorandom-generators-actually-look-like"><i class="fa fa-check"></i><b>4.2</b> What do pseudorandom generators actually look like?</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="pseudorandomness.html"><a href="pseudorandomness.html#attempt-0-the-counter-generator"><i class="fa fa-check"></i><b>4.2.1</b> Attempt 0: The counter generator</a></li>
<li class="chapter" data-level="4.2.2" data-path="pseudorandomness.html"><a href="pseudorandomness.html#attempt-1-the-linear-checksum-linear-feedback-shift-register-lfsr"><i class="fa fa-check"></i><b>4.2.2</b> Attempt 1: The linear checksum / linear feedback shift register (LFSR)</a></li>
<li class="chapter" data-level="4.2.3" data-path="pseudorandomness.html"><a href="pseudorandomness.html#from-insecurity-to-security"><i class="fa fa-check"></i><b>4.2.3</b> From insecurity to security</a></li>
<li class="chapter" data-level="4.2.4" data-path="pseudorandomness.html"><a href="pseudorandomness.html#attempt-2-linear-congruential-generators-with-dropped-bits"><i class="fa fa-check"></i><b>4.2.4</b> Attempt 2: Linear Congruential Generators with dropped bits</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="pseudorandomness.html"><a href="pseudorandomness.html#successful-examples"><i class="fa fa-check"></i><b>4.3</b> Successful examples</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="pseudorandomness.html"><a href="pseudorandomness.html#case-study-1-subset-sum-generator"><i class="fa fa-check"></i><b>4.3.1</b> Case Study 1: Subset Sum Generator</a></li>
<li class="chapter" data-level="4.3.2" data-path="pseudorandomness.html"><a href="pseudorandomness.html#case-study-2-rc4"><i class="fa fa-check"></i><b>4.3.2</b> Case Study 2: RC4</a></li>
<li class="chapter" data-level="4.3.3" data-path="pseudorandomness.html"><a href="pseudorandomness.html#case-study-3-blum-blum-and-shub"><i class="fa fa-check"></i><b>4.3.3</b> Case Study 3: Blum, Blum and Shub</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="pseudorandomness.html"><a href="pseudorandomness.html#non-constructive-existence-of-pseudorandom-generators"><i class="fa fa-check"></i><b>4.4</b> Non-constructive existence of pseudorandom generators</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html"><i class="fa fa-check"></i><b>5</b> Pseudorandom functions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#one-time-passwords-e.g.-google-authenticator-rsa-id-etc."><i class="fa fa-check"></i><b>5.1</b> One time passwords (e.g. Google Authenticator, RSA ID, etc.)</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#how-do-pseudorandom-functions-help-in-the-login-problem"><i class="fa fa-check"></i><b>5.1.1</b> How do pseudorandom functions help in the login problem?</a></li>
<li class="chapter" data-level="5.1.2" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#modifying-input-and-output-lengths-of-prfs"><i class="fa fa-check"></i><b>5.1.2</b> Modifying input and output lengths of PRFs</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#message-authentication-codes"><i class="fa fa-check"></i><b>5.2</b> Message Authentication Codes</a></li>
<li class="chapter" data-level="5.3" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#macs-from-prfs"><i class="fa fa-check"></i><b>5.3</b> MACs from PRFs</a></li>
<li class="chapter" data-level="5.4" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#arbitrary-input-length-extension-for-macs-and-prfs"><i class="fa fa-check"></i><b>5.4</b> Arbitrary input length extension for MACs and PRFs</a></li>
<li class="chapter" data-level="5.5" data-path="pseudorandom-functions.html"><a href="pseudorandom-functions.html#aside-natural-proofs"><i class="fa fa-check"></i><b>5.5</b> Aside: natural proofs</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><i class="fa fa-check"></i><b>6</b> Pseudorandom functions from pseudorandom generators and CPA security</a>
<ul>
<li class="chapter" data-level="6.1" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html#securely-encrypting-many-messages---chosen-plaintext-security"><i class="fa fa-check"></i><b>6.1</b> Securely encrypting many messages - chosen plaintext security</a></li>
<li class="chapter" data-level="6.2" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html#pseudorandom-permutations-block-ciphers"><i class="fa fa-check"></i><b>6.2</b> Pseudorandom permutations / block ciphers</a></li>
<li class="chapter" data-level="6.3" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html#encryption-modes"><i class="fa fa-check"></i><b>6.3</b> Encryption modes</a></li>
<li class="chapter" data-level="6.4" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html#optional-aside-broadcast-encryption"><i class="fa fa-check"></i><b>6.4</b> Optional, Aside: Broadcast Encryption</a></li>
<li class="chapter" data-level="6.5" data-path="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html"><a href="pseudorandom-functions-from-pseudorandom-generators-and-cpa-security.html#reading-comprehension-exercises"><i class="fa fa-check"></i><b>6.5</b> Reading comprehension exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html"><i class="fa fa-check"></i><b>7</b> Chosen Ciphertext Security</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#short-recap"><i class="fa fa-check"></i><b>7.1</b> Short recap</a></li>
<li class="chapter" data-level="7.2" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#going-beyond-cpa"><i class="fa fa-check"></i><b>7.2</b> Going beyond CPA</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#example-the-wired-equivalence-privacy-wep"><i class="fa fa-check"></i><b>7.2.1</b> Example: The Wired Equivalence Privacy (WEP)</a></li>
<li class="chapter" data-level="7.2.2" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#chosen-ciphertext-security-1"><i class="fa fa-check"></i><b>7.2.2</b> Chosen ciphertext security</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#constructing-cca-secure-encryption"><i class="fa fa-check"></i><b>7.3</b> Constructing CCA secure encryption</a></li>
<li class="chapter" data-level="7.4" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#simplified-gcm-encryption"><i class="fa fa-check"></i><b>7.4</b> (Simplified) GCM encryption</a></li>
<li class="chapter" data-level="7.5" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#padding-chopping-and-their-pitfalls-the-buffer-overflow-of-cryptography"><i class="fa fa-check"></i><b>7.5</b> Padding, chopping, and their pitfalls: the “buffer overflow” of cryptography</a></li>
<li class="chapter" data-level="7.6" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#chosen-ciphertext-attack-as-implementing-metaphors"><i class="fa fa-check"></i><b>7.6</b> Chosen ciphertext attack as implementing metaphors</a></li>
<li class="chapter" data-level="7.7" data-path="chosen-ciphertext-security.html"><a href="chosen-ciphertext-security.html#reading-comprehension-exercises-1"><i class="fa fa-check"></i><b>7.7</b> Reading comprehension exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hash-functions-random-oracles-and-bitcoin.html"><a href="hash-functions-random-oracles-and-bitcoin.html"><i class="fa fa-check"></i><b>8</b> Hash Functions, Random Oracles, and Bitcoin</a></li>
<li class="chapter" data-level="9" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><i class="fa fa-check"></i><b>9</b> Key derivation, protecting passwords, slow hashes, Merkle trees</a>
<ul>
<li class="chapter" data-level="9.1" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html#keys-from-passwords"><i class="fa fa-check"></i><b>9.1</b> Keys from passwords</a></li>
<li class="chapter" data-level="9.2" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html#merkle-trees-and-verifying-storage."><i class="fa fa-check"></i><b>9.2</b> Merkle trees and verifying storage.</a></li>
<li class="chapter" data-level="9.3" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html#proofs-of-retrievability"><i class="fa fa-check"></i><b>9.3</b> Proofs of Retrievability</a></li>
<li class="chapter" data-level="9.4" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html#entropy-extraction"><i class="fa fa-check"></i><b>9.4</b> Entropy extraction</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html"><a href="key-derivation-protecting-passwords-slow-hashes-merkle-trees.html#forward-and-backward-secrecy"><i class="fa fa-check"></i><b>9.4.1</b> Forward and backward secrecy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html"><i class="fa fa-check"></i><b>10</b> Public key cryptography</a>
<ul>
<li class="chapter" data-level="10.1" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#private-key-crypto-recap"><i class="fa fa-check"></i><b>10.1</b> Private key crypto recap</a></li>
<li class="chapter" data-level="10.2" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#public-key-encryptions-definition"><i class="fa fa-check"></i><b>10.2</b> Public Key Encryptions: Definition</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#the-obfuscation-paradigm"><i class="fa fa-check"></i><b>10.2.1</b> The obfuscation paradigm</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#some-concrete-candidates"><i class="fa fa-check"></i><b>10.3</b> Some concrete candidates:</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#diffie-hellman-encryption-aka-el-gamal"><i class="fa fa-check"></i><b>10.3.1</b> Diffie-Hellman Encryption (aka El-Gamal)</a></li>
<li class="chapter" data-level="10.3.2" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#sampling-random-primes"><i class="fa fa-check"></i><b>10.3.2</b> Sampling random primes</a></li>
<li class="chapter" data-level="10.3.3" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#a-little-bit-of-group-theory."><i class="fa fa-check"></i><b>10.3.3</b> A little bit of group theory.</a></li>
<li class="chapter" data-level="10.3.4" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#digital-signatures"><i class="fa fa-check"></i><b>10.3.4</b> Digital Signatures</a></li>
<li class="chapter" data-level="10.3.5" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#the-digital-signature-algorithm-dsa"><i class="fa fa-check"></i><b>10.3.5</b> The Digital Signature Algorithm (DSA)</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#putting-everything-together---security-in-practice."><i class="fa fa-check"></i><b>10.4</b> Putting everything together - security in practice.</a></li>
<li class="chapter" data-level="10.5" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#appendix-an-alternative-proof-of-the-density-of-primes"><i class="fa fa-check"></i><b>10.5</b> Appendix: An alternative proof of the density of primes</a></li>
<li class="chapter" data-level="10.6" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#additional-group-theory-exercises-and-proofs"><i class="fa fa-check"></i><b>10.6</b> Additional Group Theory Exercises and Proofs</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="public-key-cryptography.html"><a href="public-key-cryptography.html#solved-exercises"><i class="fa fa-check"></i><b>10.6.1</b> Solved exercises:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html"><i class="fa fa-check"></i><b>11</b> Concrete candidates for public key crypto</a>
<ul>
<li class="chapter" data-level="11.1" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#some-number-theory."><i class="fa fa-check"></i><b>11.1</b> Some number theory.</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#primaliy-testing"><i class="fa fa-check"></i><b>11.1.1</b> Primaliy testing</a></li>
<li class="chapter" data-level="11.1.2" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#fields"><i class="fa fa-check"></i><b>11.1.2</b> Fields</a></li>
<li class="chapter" data-level="11.1.3" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#chinese-remainder-theorem"><i class="fa fa-check"></i><b>11.1.3</b> Chinese remainder theorem</a></li>
<li class="chapter" data-level="11.1.4" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#the-rsa-and-rabin-functions"><i class="fa fa-check"></i><b>11.1.4</b> The RSA and Rabin functions</a></li>
<li class="chapter" data-level="11.1.5" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#abstraction-trapdoor-permutations"><i class="fa fa-check"></i><b>11.1.5</b> Abstraction: trapdoor permutations</a></li>
<li class="chapter" data-level="11.1.6" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#public-key-encryption-from-trapdoor-permutations"><i class="fa fa-check"></i><b>11.1.6</b> Public key encryption from trapdoor permutations</a></li>
<li class="chapter" data-level="11.1.7" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#digital-signatures-from-trapdoor-permutations"><i class="fa fa-check"></i><b>11.1.7</b> Digital signatures from trapdoor permutations</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#hardcore-bits-and-security-without-random-oracles"><i class="fa fa-check"></i><b>11.2</b> Hardcore bits and security without random oracles</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="concrete-candidates-for-public-key-crypto.html"><a href="concrete-candidates-for-public-key-crypto.html#extending-to-more-than-one-hardcore-bit"><i class="fa fa-check"></i><b>11.2.1</b> Extending to more than one hardcore bit</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html"><i class="fa fa-check"></i><b>12</b> Lattice based cryptography</a>
<ul>
<li class="chapter" data-level="12.0.1" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#quick-linear-algebra-recap"><i class="fa fa-check"></i><b>12.0.1</b> Quick linear algebra recap</a></li>
<li class="chapter" data-level="12.1" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#a-world-without-gaussian-elimination"><i class="fa fa-check"></i><b>12.1</b> A world without Gaussian elimination</a></li>
<li class="chapter" data-level="12.2" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#security-in-the-real-world."><i class="fa fa-check"></i><b>12.2</b> Security in the real world.</a></li>
<li class="chapter" data-level="12.3" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#search-to-decision"><i class="fa fa-check"></i><b>12.3</b> Search to decision</a></li>
<li class="chapter" data-level="12.4" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#lweencsec"><i class="fa fa-check"></i><b>12.4</b> An LWE based encryption scheme</a></li>
<li class="chapter" data-level="12.5" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#but-what-are-lattices"><i class="fa fa-check"></i><b>12.5</b> But what are lattices?</a></li>
<li class="chapter" data-level="12.6" data-path="lattice-based-cryptography.html"><a href="lattice-based-cryptography.html#ring-based-lattices"><i class="fa fa-check"></i><b>12.6</b> Ring based lattices</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html"><i class="fa fa-check"></i><b>13</b> Establishing secure connections over insecure channels</a>
<ul>
<li class="chapter" data-level="13.1" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#cryptographys-obsession-with-adjectives."><i class="fa fa-check"></i><b>13.1</b> Cryptography’s obsession with adjectives.</a></li>
<li class="chapter" data-level="13.2" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#basic-key-exchange-protocol"><i class="fa fa-check"></i><b>13.2</b> Basic Key Exchange protocol</a></li>
<li class="chapter" data-level="13.3" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#authenticated-key-exchange"><i class="fa fa-check"></i><b>13.3</b> Authenticated key exchange</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#bleichenbachers-attack-on-rsa-pkcs-v1.5-and-ssl-v3.0"><i class="fa fa-check"></i><b>13.3.1</b> Bleichenbacher’s attack on RSA PKCS V1.5 and SSL V3.0</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#chosen-ciphertext-attack-security-for-public-key-cryptography"><i class="fa fa-check"></i><b>13.4</b> Chosen ciphertext attack security for public key cryptography</a></li>
<li class="chapter" data-level="13.5" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#cca-secure-public-key-encryption-in-the-random-oracle-model"><i class="fa fa-check"></i><b>13.5</b> CCA secure public key encryption in the Random Oracle Model</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#defining-secure-authenticated-key-exchange"><i class="fa fa-check"></i><b>13.5.1</b> Defining secure authenticated key exchange</a></li>
<li class="chapter" data-level="13.5.2" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#the-compiler-approach-for-authenticated-key-exchange"><i class="fa fa-check"></i><b>13.5.2</b> The compiler approach for authenticated key exchange</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#password-authenticated-key-exchange."><i class="fa fa-check"></i><b>13.6</b> Password authenticated key exchange.</a></li>
<li class="chapter" data-level="13.7" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#client-to-client-key-exchange-for-secure-text-messaging---zrtp-otr-textsecure"><i class="fa fa-check"></i><b>13.7</b> Client to client key exchange for secure text messaging - ZRTP, OTR, TextSecure</a></li>
<li class="chapter" data-level="13.8" data-path="establishing-secure-connections-over-insecure-channels.html"><a href="establishing-secure-connections-over-insecure-channels.html#heartbleed-and-logjam-attacks"><i class="fa fa-check"></i><b>13.8</b> Heartbleed and logjam attacks</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html"><i class="fa fa-check"></i><b>14</b> Zero knowledge proofs</a>
<ul>
<li class="chapter" data-level="14.1" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#applications-for-zero-knowledge-proofs."><i class="fa fa-check"></i><b>14.1</b> Applications for zero knowledge proofs.</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#nuclear-disarmament"><i class="fa fa-check"></i><b>14.1.1</b> Nuclear disarmament</a></li>
<li class="chapter" data-level="14.1.2" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#voting"><i class="fa fa-check"></i><b>14.1.2</b> Voting</a></li>
<li class="chapter" data-level="14.1.3" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#more-applications"><i class="fa fa-check"></i><b>14.1.3</b> More applications</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#defining-and-constructing-zero-knowledge-proofs"><i class="fa fa-check"></i><b>14.2</b> Defining and constructing zero knowledge proofs</a></li>
<li class="chapter" data-level="14.3" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#defining-zero-knowledge"><i class="fa fa-check"></i><b>14.3</b> Defining zero knowledge</a></li>
<li class="chapter" data-level="14.4" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#zero-knowledge-proof-for-hamiltonicity."><i class="fa fa-check"></i><b>14.4</b> Zero knowledge proof for Hamiltonicity.</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#why-is-this-interesting"><i class="fa fa-check"></i><b>14.4.1</b> Why is this interesting?</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#parallel-repetition-and-turning-zero-knowledge-proofs-to-signatures."><i class="fa fa-check"></i><b>14.5</b> Parallel repetition and turning zero knowledge proofs to signatures.</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="zero-knowledge-proofs.html"><a href="zero-knowledge-proofs.html#bonus-features-of-zero-knowledge"><i class="fa fa-check"></i><b>14.5.1</b> “Bonus features” of zero knowledge</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="chapfheone.html"><a href="chapfheone.html"><i class="fa fa-check"></i><b>15</b> Fully homomorphic encryption: Introduction and bootstrapping</a>
<ul>
<li class="chapter" data-level="15.1" data-path="chapfheone.html"><a href="chapfheone.html#defining-fully-homomorphic-encryption"><i class="fa fa-check"></i><b>15.1</b> Defining fully homomorphic encryption</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="chapfheone.html"><a href="chapfheone.html#another-application-fully-homomorphic-encryption-for-verifying-computation"><i class="fa fa-check"></i><b>15.1.1</b> Another application: fully homomorphic encryption for verifying computation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="chapfheone.html"><a href="chapfheone.html#example-an-xor-homomorphic-encryption"><i class="fa fa-check"></i><b>15.2</b> Example: An XOR homomorphic encryption</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="chapfheone.html"><a href="chapfheone.html#abstraction-a-trapdoor-pseudorandom-generator."><i class="fa fa-check"></i><b>15.2.1</b> Abstraction: A trapdoor pseudorandom generator.</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="chapfheone.html"><a href="chapfheone.html#from-linear-homomorphism-to-full-homomorphism"><i class="fa fa-check"></i><b>15.3</b> From linear homomorphism to full homomorphism</a></li>
<li class="chapter" data-level="15.4" data-path="chapfheone.html"><a href="chapfheone.html#bootstrapping-fully-homomorphic-escape-velocity"><i class="fa fa-check"></i><b>15.4</b> Bootstrapping: Fully Homomorphic “escape velocity”</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="chapfheone.html"><a href="chapfheone.html#radioactive-legos-analogy"><i class="fa fa-check"></i><b>15.4.1</b> Radioactive legos analogy</a></li>
<li class="chapter" data-level="15.4.2" data-path="chapfheone.html"><a href="chapfheone.html#proving-the-bootstrapping-theorem"><i class="fa fa-check"></i><b>15.4.2</b> Proving the bootstrapping theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="chapfhetwo.html"><a href="chapfhetwo.html"><i class="fa fa-check"></i><b>16</b> Fully homomorphic encryption: Construction</a>
<ul>
<li class="chapter" data-level="16.1" data-path="chapfhetwo.html"><a href="chapfhetwo.html#prelude-from-vectors-to-matrices"><i class="fa fa-check"></i><b>16.1</b> Prelude: from vectors to matrices</a></li>
<li class="chapter" data-level="16.2" data-path="chapfhetwo.html"><a href="chapfhetwo.html#real-world-partially-homomorphic-encryption"><i class="fa fa-check"></i><b>16.2</b> Real world partially homomorphic encryption</a></li>
<li class="chapter" data-level="16.3" data-path="chapfhetwo.html"><a href="chapfhetwo.html#noise-management-via-encoding"><i class="fa fa-check"></i><b>16.3</b> Noise management via encoding</a></li>
<li class="chapter" data-level="16.4" data-path="chapfhetwo.html"><a href="chapfhetwo.html#putting-it-all-together"><i class="fa fa-check"></i><b>16.4</b> Putting it all together</a></li>
<li class="chapter" data-level="16.5" data-path="chapfhetwo.html"><a href="chapfhetwo.html#analysis-of-our-scheme"><i class="fa fa-check"></i><b>16.5</b> Analysis of our scheme</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="chapfhetwo.html"><a href="chapfhetwo.html#correctness"><i class="fa fa-check"></i><b>16.5.1</b> Correctness</a></li>
<li class="chapter" data-level="16.5.2" data-path="chapfhetwo.html"><a href="chapfhetwo.html#cpa-security"><i class="fa fa-check"></i><b>16.5.2</b> CPA Security</a></li>
<li class="chapter" data-level="16.5.3" data-path="chapfhetwo.html"><a href="chapfhetwo.html#homomorphism"><i class="fa fa-check"></i><b>16.5.3</b> Homomorphism</a></li>
<li class="chapter" data-level="16.5.4" data-path="chapfhetwo.html"><a href="chapfhetwo.html#shallow-decryption-circuit"><i class="fa fa-check"></i><b>16.5.4</b> Shallow decryption circuit</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="chapfhetwo.html"><a href="chapfhetwo.html#advanced-topics"><i class="fa fa-check"></i><b>16.6</b> Advanced topics:</a>
<ul>
<li class="chapter" data-level="16.6.1" data-path="chapfhetwo.html"><a href="chapfhetwo.html#fully-homomorphic-encryption-for-approximate-computation-over-the-real-numbers-ckks"><i class="fa fa-check"></i><b>16.6.1</b> Fully homomorphic encryption for approximate computation over the real numbers: CKKS</a></li>
<li class="chapter" data-level="16.6.2" data-path="chapfhetwo.html"><a href="chapfhetwo.html#bandwidth-efficient-fully-homomorphic-encryption-gh"><i class="fa fa-check"></i><b>16.6.2</b> Bandwidth efficient fully homomorphic encryption GH</a></li>
<li class="chapter" data-level="16.6.3" data-path="chapfhetwo.html"><a href="chapfhetwo.html#using-fully-homomorphic-encryption-to-achieve-private-information-retrieval."><i class="fa fa-check"></i><b>16.6.3</b> Using fully homomorphic encryption to achieve private information retrieval.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="sfeonechap.html"><a href="sfeonechap.html"><i class="fa fa-check"></i><b>17</b> Multiparty secure computation I: Definition and Honest-But-Curious to Malicious complier</a>
<ul>
<li class="chapter" data-level="17.1" data-path="sfeonechap.html"><a href="sfeonechap.html#ideal-vs.-real-model-security."><i class="fa fa-check"></i><b>17.1</b> Ideal vs. Real Model Security.</a></li>
<li class="chapter" data-level="17.2" data-path="sfeonechap.html"><a href="sfeonechap.html#formally-defining-secure-multiparty-computation"><i class="fa fa-check"></i><b>17.2</b> Formally defining secure multiparty computation</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="sfeonechap.html"><a href="sfeonechap.html#first-attempt-a-slightly-too-ideal-definition"><i class="fa fa-check"></i><b>17.2.1</b> First attempt: a slightly “too ideal” definition</a></li>
<li class="chapter" data-level="17.2.2" data-path="sfeonechap.html"><a href="sfeonechap.html#allowing-for-aborts"><i class="fa fa-check"></i><b>17.2.2</b> Allowing for aborts</a></li>
<li class="chapter" data-level="17.2.3" data-path="sfeonechap.html"><a href="sfeonechap.html#some-comments"><i class="fa fa-check"></i><b>17.2.3</b> Some comments:</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="sfeonechap.html"><a href="sfeonechap.html#example-second-price-auction-using-bitcoin"><i class="fa fa-check"></i><b>17.3</b> Example: Second price auction using bitcoin</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="sfeonechap.html"><a href="sfeonechap.html#another-example-distributed-and-threshold-cryptography"><i class="fa fa-check"></i><b>17.3.1</b> Another example: distributed and threshold cryptography</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="sfeonechap.html"><a href="sfeonechap.html#proving-the-fundamental-theorem"><i class="fa fa-check"></i><b>17.4</b> Proving the fundamental theorem:</a></li>
<li class="chapter" data-level="17.5" data-path="sfeonechap.html"><a href="sfeonechap.html#hbctomalred"><i class="fa fa-check"></i><b>17.5</b> Malicious to honest but curious reduction</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="sfeonechap.html"><a href="sfeonechap.html#handling-probabilistic-strategies"><i class="fa fa-check"></i><b>17.5.1</b> Handling probabilistic strategies:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="sfetwochap.html"><a href="sfetwochap.html"><i class="fa fa-check"></i><b>18</b> Multiparty secure computation II: Construction using Fully Homomorphic Encryption</a>
<ul>
<li class="chapter" data-level="18.1" data-path="sfetwochap.html"><a href="sfetwochap.html#constructing-2-party-honest-but-curious-computation-from-fully-homomorphic-encryption"><i class="fa fa-check"></i><b>18.1</b> Constructing 2 party honest but curious computation from fully homomorphic encryption</a></li>
<li class="chapter" data-level="18.2" data-path="sfetwochap.html"><a href="sfetwochap.html#achieving-circuit-privacy-in-a-fully-homomorphic-encryption"><i class="fa fa-check"></i><b>18.2</b> Achieving circuit privacy in a fully homomorphic encryption</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="sfetwochap.html"><a href="sfetwochap.html#bottom-line-a-two-party-secure-computation-protocol"><i class="fa fa-check"></i><b>18.2.1</b> Bottom line: A two party secure computation protocol</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="sfetwochap.html"><a href="sfetwochap.html#beyond-two-parties"><i class="fa fa-check"></i><b>18.3</b> Beyond two parties</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html"><i class="fa fa-check"></i><b>19</b> Quantum computing and cryptography I</a>
<ul>
<li class="chapter" data-level="19.1" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#the-double-slit-experiment"><i class="fa fa-check"></i><b>19.1</b> The double slit experiment</a></li>
<li class="chapter" data-level="19.2" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#quantum-amplitudes"><i class="fa fa-check"></i><b>19.2</b> Quantum amplitudes</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#quantum-computing-and-computation---an-executive-summary."><i class="fa fa-check"></i><b>19.2.1</b> Quantum computing and computation - an executive summary.</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#quantum-101"><i class="fa fa-check"></i><b>19.3</b> Quantum 101</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#physically-realizing-quantum-computation"><i class="fa fa-check"></i><b>19.3.1</b> Physically realizing quantum computation</a></li>
<li class="chapter" data-level="19.3.2" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#bra-ket-notation"><i class="fa fa-check"></i><b>19.3.2</b> Bra-ket notation</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#bells-inequality"><i class="fa fa-check"></i><b>19.4</b> Bell’s Inequality</a></li>
<li class="chapter" data-level="19.5" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#analysis-of-bells-inequality"><i class="fa fa-check"></i><b>19.5</b> Analysis of Bell’s Inequality</a></li>
<li class="chapter" data-level="19.6" data-path="quantum-computing-and-cryptography-i.html"><a href="quantum-computing-and-cryptography-i.html#grovers-algorithm"><i class="fa fa-check"></i><b>19.6</b> Grover’s Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html"><i class="fa fa-check"></i><b>20</b> Quantum computing and cryptography II</a>
<ul>
<li class="chapter" data-level="20.1" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#from-order-finding-to-factoring-and-discrete-log"><i class="fa fa-check"></i><b>20.1</b> From order finding to factoring and discrete log</a></li>
<li class="chapter" data-level="20.2" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#finding-periods-of-a-function-simons-algorithm"><i class="fa fa-check"></i><b>20.2</b> Finding periods of a function: Simon’s Algorithm</a></li>
<li class="chapter" data-level="20.3" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#from-simon-to-shor"><i class="fa fa-check"></i><b>20.3</b> From Simon to Shor</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#the-fourier-transform-over-mathbbz_m"><i class="fa fa-check"></i><b>20.3.1</b> The Fourier transform over <span class="math inline">\(\mathbb{Z}_m\)</span></a></li>
<li class="chapter" data-level="20.3.2" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#quantum-fourier-transform-over-mathbbz_m"><i class="fa fa-check"></i><b>20.3.2</b> Quantum Fourier Transform over <span class="math inline">\(\mathbb{Z}_m\)</span></a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#shor鈥檚-order-finding-algorithm."><i class="fa fa-check"></i><b>20.4</b> Shor鈥檚 Order-Finding Algorithm.</a>
<ul>
<li class="chapter" data-level="20.4.1" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#analysis-the-case-that-rm"><i class="fa fa-check"></i><b>20.4.1</b> Analysis: the case that <span class="math inline">\(r|m\)</span></a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#rational-approximation-of-real-numbers"><i class="fa fa-check"></i><b>20.5</b> Rational approximation of real numbers</a>
<ul>
<li class="chapter" data-level="20.5.1" data-path="quantum-computing-and-cryptography-ii.html"><a href="quantum-computing-and-cryptography-ii.html#quantum-cryptography"><i class="fa fa-check"></i><b>20.5.1</b> Quantum cryptography</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="software-obfuscation.html"><a href="software-obfuscation.html"><i class="fa fa-check"></i><b>21</b> Software Obfuscation</a>
<ul>
<li class="chapter" data-level="21.1" data-path="software-obfuscation.html"><a href="software-obfuscation.html#witness-encryption"><i class="fa fa-check"></i><b>21.1</b> Witness encryption</a></li>
<li class="chapter" data-level="21.2" data-path="software-obfuscation.html"><a href="software-obfuscation.html#deniable-encryption"><i class="fa fa-check"></i><b>21.2</b> Deniable encryption</a></li>
<li class="chapter" data-level="21.3" data-path="software-obfuscation.html"><a href="software-obfuscation.html#functional-encryption"><i class="fa fa-check"></i><b>21.3</b> Functional encryption</a></li>
<li class="chapter" data-level="21.4" data-path="software-obfuscation.html"><a href="software-obfuscation.html#the-software-patch-problem"><i class="fa fa-check"></i><b>21.4</b> The software patch problem</a></li>
<li class="chapter" data-level="21.5" data-path="software-obfuscation.html"><a href="software-obfuscation.html#software-obfuscation-1"><i class="fa fa-check"></i><b>21.5</b> Software obfuscation</a></li>
<li class="chapter" data-level="21.6" data-path="software-obfuscation.html"><a href="software-obfuscation.html#applications-of-obfuscation"><i class="fa fa-check"></i><b>21.6</b> Applications of obfuscation</a></li>
<li class="chapter" data-level="21.7" data-path="software-obfuscation.html"><a href="software-obfuscation.html#impossibility-of-obfuscation"><i class="fa fa-check"></i><b>21.7</b> Impossibility of obfuscation</a>
<ul>
<li class="chapter" data-level="21.7.1" data-path="software-obfuscation.html"><a href="software-obfuscation.html#proof-of-impossibility-of-vbb-obfuscation"><i class="fa fa-check"></i><b>21.7.1</b> Proof of impossibility of VBB obfuscation</a></li>
</ul></li>
<li class="chapter" data-level="21.8" data-path="software-obfuscation.html"><a href="software-obfuscation.html#indistinguishability-obfuscation"><i class="fa fa-check"></i><b>21.8</b> Indistinguishability obfuscation</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="more-obfuscation-exotic-encryptions.html"><a href="more-obfuscation-exotic-encryptions.html"><i class="fa fa-check"></i><b>22</b> More obfuscation, exotic encryptions</a>
<ul>
<li class="chapter" data-level="22.1" data-path="more-obfuscation-exotic-encryptions.html"><a href="more-obfuscation-exotic-encryptions.html#slower-weaker-less-securer"><i class="fa fa-check"></i><b>22.1</b> Slower, weaker, less securer</a></li>
<li class="chapter" data-level="22.2" data-path="more-obfuscation-exotic-encryptions.html"><a href="more-obfuscation-exotic-encryptions.html#how-to-get-ibe-from-pairing-based-assumptions."><i class="fa fa-check"></i><b>22.2</b> How to get IBE from pairing based assumptions.</a></li>
<li class="chapter" data-level="22.3" data-path="more-obfuscation-exotic-encryptions.html"><a href="more-obfuscation-exotic-encryptions.html#beyond-pairing-based-cryptography"><i class="fa fa-check"></i><b>22.3</b> Beyond pairing based cryptography</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="anonymous-communication.html"><a href="anonymous-communication.html"><i class="fa fa-check"></i><b>23</b> Anonymous communication</a>
<ul>
<li class="chapter" data-level="23.1" data-path="anonymous-communication.html"><a href="anonymous-communication.html#steganography"><i class="fa fa-check"></i><b>23.1</b> Steganography</a></li>
<li class="chapter" data-level="23.2" data-path="anonymous-communication.html"><a href="anonymous-communication.html#anonymous-routing"><i class="fa fa-check"></i><b>23.2</b> Anonymous routing</a></li>
<li class="chapter" data-level="23.3" data-path="anonymous-communication.html"><a href="anonymous-communication.html#tor"><i class="fa fa-check"></i><b>23.3</b> Tor</a></li>
<li class="chapter" data-level="23.4" data-path="anonymous-communication.html"><a href="anonymous-communication.html#telex"><i class="fa fa-check"></i><b>23.4</b> Telex</a></li>
<li class="chapter" data-level="23.5" data-path="anonymous-communication.html"><a href="anonymous-communication.html#riposte"><i class="fa fa-check"></i><b>23.5</b> Riposte</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html"><i class="fa fa-check"></i><b>24</b> Ethical, moral, and policy dimensions to cryptography</a>
<ul>
<li class="chapter" data-level="24.1" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html#reading-prior-to-lecture"><i class="fa fa-check"></i><b>24.1</b> Reading prior to lecture:</a></li>
<li class="chapter" data-level="24.2" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html#case-studies."><i class="fa fa-check"></i><b>24.2</b> Case studies.</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html#the-snowden-revelations"><i class="fa fa-check"></i><b>24.2.1</b> The Snowden revelations</a></li>
<li class="chapter" data-level="24.2.2" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html#fbi-vs-apple-case"><i class="fa fa-check"></i><b>24.2.2</b> FBI vs Apple case</a></li>
<li class="chapter" data-level="24.2.3" data-path="ethical-moral-and-policy-dimensions-to-cryptography.html"><a href="ethical-moral-and-policy-dimensions-to-cryptography.html#juniper-backdoor-case-and-the-opm-break-in"><i class="fa fa-check"></i><b>24.2.3</b> Juniper backdoor case and the OPM break-in</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="course-recap.html"><a href="course-recap.html"><i class="fa fa-check"></i><b>25</b> Course recap</a>
<ul>
<li class="chapter" data-level="25.1" data-path="course-recap.html"><a href="course-recap.html#some-things-we-did-not-cover"><i class="fa fa-check"></i><b>25.1</b> Some things we did not cover</a></li>
<li class="chapter" data-level="25.2" data-path="course-recap.html"><a href="course-recap.html#what-i-hope-you-learned"><i class="fa fa-check"></i><b>25.2</b> What I hope you learned</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Minimal Book Example</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mathematical-background" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Mathematical Background<a href="mathematical-background.html#mathematical-background" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This is a brief review of some mathematical tools, and especially probability
theory, that we will use in this course.
See also the <a href="http://www.introtcs.org/public/lec_00_1_math_background.html">mathematical background</a> and <a href="http://www.introtcs.org/public/lec_15_probability.html">probability</a> lectures in my <a href="http://www.introtcs.org/">Notes on Introduction to Theoretical Computer Science</a>, which share much of the following text.</p>
<p>At Harvard, much of this material (and more) is taught in Stat 110 “Introduction
to Probability”, CS20 “Discrete Mathematics”, and AM107 “Graph Theory and
Combinatorics”. Some good sources for this material are the lecture notes by
Papadimitriou and Vazirani (see home page of Umesh Vaziarani), Lehman, Leighton
and Meyer from MIT Course 6.042 “Mathematics For Computer Science” (Chapters 1-2
and 14 to 19 are particularly relevant), and the Berkeley course CS 70. The mathematical tool we use most often
is discrete probability. The “Probabilistic Method” book by Alon and Spencer is
a great resource in this area. Also, the books of Mitzenmacher and Upfal and
Prabhakar and Raghavan cover probability from a more algorithmic perspective.
For an excellent popular discussion of some of the mathematical concepts we’ll talk about
see the book <em>“How Not to Be Wrong”</em> by Jordan Ellenberg.</p>
<p>Although knowledge of algorithms is not strictly necessary, it would be quite
useful. Students who did not take an algorithms class such as CS 124 might want to look at
(1) Corman, Leiserson, Rivest and Smith, (2) Dasgupte, Papadimitriou and
Vaziarni, or (3) Kleinberg and Tardos. We do not require prior knowledge of
complexity or computability but some basic familiarity could be useful.
Students who did not take a theory of computation class such as CS 121
might want to look at my lecture notes or the first 2 chapters of my book with Arora.</p>
<div id="a-quick-overview-of-mathematical-prerequisites" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> A quick overview of mathematical prerequisites<a href="mathematical-background.html#a-quick-overview-of-mathematical-prerequisites" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main notions we will use in this course are the following:</p>
<ul>
<li><p><strong>Proofs:</strong> First and foremost, this course will involve a heavy dose of formal mathematical reasoning, which includes mathematical <em>definitions</em>, <em>statements</em>, and <em>proofs</em>.</p></li>
<li><p><strong>Sets and functions:</strong> We will assume familiarity with basic notions of sets and operations on sets such as union (denoted <span class="math inline">\(\cup\)</span>), intersection (denoted <span class="math inline">\(\cap\)</span>), and set subtraction
(denoted <span class="math inline">\(\setminus\)</span>). We denote by <span class="math inline">\(|A|\)</span> the size of the set <span class="math inline">\(A\)</span>. We also assume familiarity with functions, and notions such as one-to-one (injective) functions and onto (surjective) functions. If <span class="math inline">\(f\)</span> is a function from a set <span class="math inline">\(A\)</span> to a set <span class="math inline">\(B\)</span>, we denote this by <span class="math inline">\(f:A\rightarrow B\)</span>. If <span class="math inline">\(f\)</span> is one-to-one then this implies that <span class="math inline">\(|A| \leq |B|\)</span>. If <span class="math inline">\(f\)</span> is onto then <span class="math inline">\(|A| \geq |B|\)</span>. If <span class="math inline">\(f\)</span> is a permutation/bijection (e.g., one-to-one <em>and</em> onto) then this implies that <span class="math inline">\(|A|=|B|\)</span>.</p></li>
<li><p><strong>Big Oh notation:</strong> If <span class="math inline">\(f,g\)</span> are two functions from <span class="math inline">\({\mathbb{N}}\)</span> to <span class="math inline">\({\mathbb{N}}\)</span>, then (1) <span class="math inline">\(f = O(g)\)</span> if there exists a
constant <span class="math inline">\(c\)</span> such that <span class="math inline">\(f(n) \leq c\cdot g(n)\)</span> for every sufficiently large <span class="math inline">\(n\)</span>, (2) <span class="math inline">\(f = \Omega(g)\)</span> if <span class="math inline">\(g=O(f)\)</span>, (3) <span class="math inline">\(f = \Theta(g)\)</span> is <span class="math inline">\(f=O(g)\)</span> and <span class="math inline">\(g=O(f)\)</span>, (4) <span class="math inline">\(f = o(g)\)</span> if for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math inline">\(f(n) \leq \epsilon \cdot g(n)\)</span> for every sufficiently large <span class="math inline">\(n\)</span>, and (5) <span class="math inline">\(f = \omega(g)\)</span> if <span class="math inline">\(g = o(f)\)</span>. To emphasize the input parameter, we often write <span class="math inline">\(f(n) = O(g(n))\)</span> instead of <span class="math inline">\(f = O(g)\)</span>, and use similar notation for <span class="math inline">\(o,\Omega,\omega,\Theta\)</span>. While this is only an imprecise heuristic, when you see a statement of the form <span class="math inline">\(f(n)=O(g(n))\)</span> you can often replace it in your mind by the statement <span class="math inline">\(f(n) \leq 1000g(n)\)</span> while the statement <span class="math inline">\(f(n) = \Omega(g(n))\)</span> can often be thought of as <span class="math inline">\(f(n)\geq 0.001g(n)\)</span> .</p></li>
<li><p><strong>Logical operations:</strong> The operations AND, OR, and NOT (<span class="math inline">\(\wedge,\vee,\neg\)</span>) and the quantifiers “exists” and “forall” (<span class="math inline">\(\exists\)</span>,<span class="math inline">\(\forall\)</span>).</p></li>
<li><p><strong>Tuples and strings:</strong> The notation <span class="math inline">\(\Sigma^k\)</span> and <span class="math inline">\(\Sigma^*\)</span> where <span class="math inline">\(\Sigma\)</span> is some finite set which is called the <em>alphabet</em> (quite often <span class="math inline">\(\Sigma = \{0,1\}\)</span>).</p></li>
<li><p><strong>Graphs:</strong> Undirected and directed graphs, connectivity, paths, and cycles.</p></li>
<li><p><strong>Basic combinatorics:</strong> Notions such as <span class="math inline">\(\binom{n}{k}\)</span> (the number of <span class="math inline">\(k\)</span>-sized subset of a set of size <span class="math inline">\(n\)</span>).</p></li>
<li><p><strong>Discrete probability:</strong> We will extensively use <em>probability theory</em>, and specifically probability over <em>finite</em> samples spaces such as tossing <span class="math inline">\(n\)</span> coins, including notions such as <em>random variables</em>, <em>expectation</em>, and <em>concentration</em>.</p></li>
<li><p><strong>Modular arithmetic:</strong> We will use <a href="https://en.wikipedia.org/wiki/Modular_arithmetic">modular arithmetic</a> (i.e., addition and multiplication modulo some number <span class="math inline">\(m\)</span>), and in particular talk about operations on vectors and matrices whose elements are taken modulo <span class="math inline">\(m\)</span>. If <span class="math inline">\(n\)</span> is an integer, then we denote by <span class="math inline">\(a \pmod{n}\)</span> the remainder of <span class="math inline">\(a\)</span> when divided by <span class="math inline">\(n\)</span>. <span class="math inline">\(a\pmod{n}\)</span> is the number <span class="math inline">\(r\in\{0,\ldots,n-1\}\)</span> such that <span class="math inline">\(a = kn+r\)</span> for some integer <span class="math inline">\(k\)</span>. It will be very useful that <span class="math inline">\(a\pmod{n} + b \pmod{n} = (a+b) \pmod{n}\)</span> and <span class="math inline">\(a\pmod{n} \cdot b \pmod{n} = (a\cdot b) \pmod{n}\)</span> and so modular arithmetic inherits all the rules (associativity, commutativity etc..) of integer arithmetic. If <span class="math inline">\(a,b\)</span> are positive integers then <span class="math inline">\(gcd(a,b)\)</span> is the largest integer that divides both <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. It is known that for every <span class="math inline">\(a,b\)</span> there exist (not necessarily positive) integers <span class="math inline">\(x,y\)</span> such that <span class="math inline">\(ax + by = gcd(a,b)\)</span> (it’s a good exercise to prove this on your own). In particular, if <span class="math inline">\(gcd(a,n)=1\)</span> then there exists a <em>modular inverse</em> for <span class="math inline">\(a\)</span> which is a number <span class="math inline">\(b\)</span> such that <span class="math inline">\(ab = 1 \pmod{n}\)</span>. We sometimes write <span class="math inline">\(b\)</span> as <span class="math inline">\(a^{-1} \pmod{n}\)</span>.</p></li>
<li><p><strong>Group theory, linear algebra:</strong> In later parts of the course we will need the notions of matrices, vectors, matrix multiplication and inverse, determinant, eigenvalues, and eigenvectors. These can be picked up in any basic text on linear algebra. In some parts we might also use some basic facts of group theory (finite groups only, and mostly only commutative ones). These also can be picked up as we go along, and a prior course on group theory is not necessary.</p></li>
<li><p><strong>Discrete probability:</strong> <em>Probability theory</em>, and specifically probability over <em>finite</em> samples spaces such as tossing <span class="math inline">\(n\)</span> coins is a crucial part of cryptography, since (as we’ll see) there is no secrecy without randomness.</p></li>
</ul>
</div>
<div id="mathematical-proofs" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Mathematical Proofs<a href="mathematical-background.html#mathematical-proofs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Arguably <em>the</em> mathematical prerequisite needed for this course is a certain
level of comfort with mathematical proofs. Many students tend to think of
mathematical proofs as a very formal object, like the proofs studied in school
in geometry, consisting of a sequence of axioms and statements derived from them
by very specific rules. In fact,</p>
<blockquote>
<p><em>a proof is a piece of writing meant to convince human readers that a
particular statement is true.</em></p>
</blockquote>
<p>(In this class, the particular humans you are trying to convince are me and the
teaching fellows.)</p>
<p>To write a proof of some statement X you need to follow three steps:</p>
<ol style="list-style-type: decimal">
<li><p>Make sure that you completely understand the statement X.</p></li>
<li><p>Think about X until you are able to convince <em>yourself</em> that X is true.</p></li>
<li><p>Think how to present the argument in the clearest possible way so you can
convince the reader as well.</p></li>
</ol>
<p>Like any good piece of writing, a proof should be concise and not be overly
formal or cumbersome. In fact, overuse of formalism can often be <em>detrimental</em>
to the argument since it can mask weaknesses in the argument from both the
writer and the reader. Sometimes students try to “throw the kitchen sink” at an
answer trying to list all possibly relevant facts in the hope of getting partial
credit. But a proof is a piece of writing, and a badly written proof will not
get credit even if it contains some correct elements. It is better to write a
clear proof of a partial statement. In particular, if you haven’t been able to
convince yourself that the statement is true, you should be honest about it and
explain which parts of the statement you have been able to verify and which
parts you haven’t.</p>
<div id="example-the-existence-of-infinitely-many-primes." class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Example: The existence of infinitely many primes.<a href="mathematical-background.html#example-the-existence-of-infinitely-many-primes." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the spirit of “do what I say and not what I do”, I will now demonstrate the
importance of conciseness by belaboring the point and spending several
paragraphs on a simple proof, written by Euclid around 300 BC. Recall that a
<em>prime number</em> is an integer <span class="math inline">\(p&gt;1\)</span> whose only divisors are <span class="math inline">\(p\)</span> and <span class="math inline">\(1\)</span>. Euclid’s
Theorem is the following:</p>
<blockquote>
<h1 id="infprimesthm" class="theorem" title="Infinitude of primes"></h1>
<p>There exist infinitely many primes.</p>
</blockquote>
<p>Instead of simply writing down the proof, let us try to understand how we might
figure this proof out. (If you haven’t seen this proof before, or you don’t
remember it, you might want to stop reading at this point and try to come up
with it on your own before continuing.) The first (and often most important)
step is to understand what the statement means. Saying that the number of primes
is infinite means that it is not finite. More precisely, this means that for
every natural number <span class="math inline">\(k\)</span>, there are more than <span class="math inline">\(k\)</span> primes.</p>
<p>Now that we understand what we need to prove, let us try to convince ourselves
of this fact. At first, it might seem obvious— since there are infinitely many
natural numbers, and every one of them can be factored into primes, there must
be infinitely many primes, right?</p>
<p>Wrong. Since we can multiply a prime many times with itself, a finite number of
primes can generate infinitely many numbers. Indeed the single prime <span class="math inline">\(3\)</span>
generates the infinite set of all numbers of the form <span class="math inline">\(3^n\)</span>. So, what we really
need to show is that for every finite set of primes <span class="math inline">\(\{ p_1,\ldots,p_k\}\)</span>, there
exists a number <span class="math inline">\(n\)</span> that has a prime factor outside this set.</p>
<p>Now we need to start playing around. Suppose that we had just two primes <span class="math inline">\(p\)</span> and
<span class="math inline">\(q\)</span>. How would we find a number <span class="math inline">\(n\)</span> that is not generated by <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>? If you
try to draw things on the number line, you will see that there is always some
<em>gap</em> between multiples of <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in the sense that they are never
consecutive. It is possible to prove that (in fact, it’s not a bad exercise) but
this observation already suggests a guess for what would be a number that is
divisible by neither <span class="math inline">\(p\)</span> nor <span class="math inline">\(q\)</span>, namely <span class="math inline">\(pq+1\)</span>. Indeed, the remainder of
<span class="math inline">\(n=pq+1\)</span> when dividing by either <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> would be <span class="math inline">\(1\)</span> (which in particular is
not zero). This observation generalizes and we can set <span class="math inline">\(n=pqr+1\)</span> to be a number
that is divisible neither by <span class="math inline">\(p,q\)</span> nor <span class="math inline">\(r\)</span>, and more generally <span class="math inline">\(n=p_1\cdots, p_k
+1\)</span> is not divisible by <span class="math inline">\(p_1,\ldots,p_k\)</span>.</p>
<p>Now we have convinced ourselves of the statement and it is time to think of how
to write this down in the clearest way. One issue that arises is that we want to
prove things truly from the definition of primes and first principles, and so
not assume properties of division and remainders or even the existence of a
prime factorization, without proving it. Here is what a proof could look like.
We will prove the following two lemmas:</p>
<blockquote>
<h1 id="primesfirstlem" class="lemma" title="Existence of prime divisor"></h1>
<p>For every integer <span class="math inline">\(n&gt;1\)</span>, there exists a prime <span class="math inline">\(p&gt;1\)</span> that divides
<span class="math inline">\(n\)</span>.</p>
</blockquote>
<blockquote>
<h1 id="primesseclem" class="lemma" title="Existence of co-prime"></h1>
<p>For every set of integers <span class="math inline">\(p_1,\ldots,p_k&gt;1\)</span>, there exists a number
<span class="math inline">\(n\)</span> such that none of <span class="math inline">\(p_1,\ldots,p_k\)</span> divide <span class="math inline">\(n\)</span>.</p>
</blockquote>
<p>From these two lemmas it follows that there exist infinitely many primes, since
otherwise if we let <span class="math inline">\(p_1,\ldots,p_k\)</span> be the set of all primes, then we would get
a contradiction as by combining <a href="" class="ref">primesfirstlem</a> and <a href="" class="ref">primesseclem</a> we would get a number <span class="math inline">\(n\)</span>
with a prime factor outside this set. We now prove the lemmas:</p>
<blockquote>
<h1 id="section" class="proof"></h1>
<p>Let <span class="math inline">\(n&gt;1\)</span> be a number, and let <span class="math inline">\(p\)</span> be the smallest divisor
of <span class="math inline">\(n\)</span> that is larger than <span class="math inline">\(1\)</span> (there exists such a number <span class="math inline">\(p\)</span> since <span class="math inline">\(n\)</span> divides
itself). We claim that <span class="math inline">\(p\)</span> is a prime. Indeed suppose otherwise there was some
<span class="math inline">\(1&lt; q &lt; p\)</span> that divides <span class="math inline">\(p\)</span>. Then since <span class="math inline">\(n = pc\)</span> for some integer <span class="math inline">\(c\)</span> and
<span class="math inline">\(p=qc&#39;\)</span> for some integer <span class="math inline">\(c&#39;\)</span> we’ll get that <span class="math inline">\(n=qcc&#39;\)</span> and hence <span class="math inline">\(q\)</span> divides <span class="math inline">\(n\)</span>
in contradiction to the choice of <span class="math inline">\(p\)</span> as the smallest divisor of <span class="math inline">\(n\)</span>.</p>
</blockquote>
<blockquote>
<h1 id="section-1" class="proof"></h1>
<p>Let <span class="math inline">\(n=p_1 \cdots p_k + 1\)</span> and suppose for the sake of
contradiction that there exists some <span class="math inline">\(i\)</span> such that <span class="math inline">\(n = p_i\cdot c\)</span> for some
integer <span class="math inline">\(c\)</span>. Then if we divide the equation <span class="math inline">\(n - p_1 \cdots p_k = 1\)</span> by <span class="math inline">\(p_i\)</span>
then we get <span class="math inline">\(c\)</span> minus an integer on the lefthand side, and the fraction
<span class="math inline">\(1/p_i\)</span> on the righthand side.</p>
</blockquote>
<p>This completes the proof of <a href="" class="ref">infprimesthm</a></p>
</div>
</div>
<div id="probability-and-sample-spaces" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Probability and Sample spaces<a href="mathematical-background.html#probability-and-sample-spaces" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Perhaps the main mathematical background needed in cryptography is probability
theory since, as we will see, there is no secrecy without randomness. Luckily,
we only need fairly basic notions of probability theory and in particular only
probability over finite sample spaces. If you have a good understanding of what
happens when we toss <span class="math inline">\(k\)</span> random coins, then you know most of the probability
you’ll need.</p>
<p>The discussion below is not meant to replace a course on probability theory, and if you have not seen this material before, I highly recommend you look at additional resources to get up to speed.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>The nature of randomness and probability is a topic of great philosophical, scientific and mathematical depth.
Is there actual randomness in the world, or does it proceed in a deterministic clockwork fashion from some initial conditions set at the beginning of time?
Does probability refer to our uncertainty of beliefs, or to the frequency of occurrences in repeated experiments?
How can we define probability over infinite sets?</p>
<p>These are all important questions that have been studied and debated by scientists, mathematicians, statisticians, and philosophers.
Fortunately, we will not need to deal directly with these questions here.
We will be mostly interested in the setting of tossing <span class="math inline">\(n\)</span> random, unbiased and independent coins.
Below we define the basic probabilistic objects of <em>events</em> and <em>random variables</em> when restricted to this setting.
These can be defined for much more general probabilistic experiments or <em>sample spaces</em>, and later on we will briefly discuss how this can be done. However, the <span class="math inline">\(n\)</span>-coin case is sufficient for almost everything we’ll need in this course.</p>
<p>If instead of “heads” and “tails” we encode the sides of each coin by “zero” and “one”, we can encode the result of tossing <span class="math inline">\(n\)</span> coins as a string in <span class="math inline">\(\{0,1\}^n\)</span>.
Each particular outcome <span class="math inline">\(x\in \{0,1\}^n\)</span> is obtained with probability <span class="math inline">\(2^{-n}\)</span>.
For example, if we toss three coins, then we obtain each of the 8 outcomes <span class="math inline">\(000,001,010,011,100,101,110,111\)</span> with probability <span class="math inline">\(2^{-3}=1/8\)</span> (see also <a href="" class="ref">coinexperimentfig</a>).
We can describe the experiment of tossing <span class="math inline">\(n\)</span> coins as choosing a string <span class="math inline">\(x\)</span> uniformly at random from <span class="math inline">\(\{0,1\}^n\)</span>, and hence we’ll use the shorthand <span class="math inline">\(x\sim \{0,1\}^n\)</span> for <span class="math inline">\(x\)</span> that is chosen according to this experiment.</p>
<div class="float" id="coinexperimentfig">
<img src="../figure/coinexperiment.png" class="margin" alt="The probabilistic experiment of tossing three coins corresponds to making 2\times 2 \times 2 = 8 choices, each with equal probability. In this example, the blue set corresponds to the event A = \{ x\in \{0,1\}^3 \;|\; x_0 = 0 \} where the first coin toss is equal to 0, and the pink set corresponds to the event B = \{ x\in \{0,1\}^3 \;|\; x_1 = 1 \} where the second coin toss is equal to 1 (with their intersection having a purplish color). As we can see, each of these events contains 4 elements (out of 8 total) and so has probability 1/2. The intersection of A and B contains two elements, and so the probability that both of these events occur is 2/8 = 1/4." />
<div class="figcaption">The probabilistic experiment of tossing three coins corresponds to making <span class="math inline">\(2\times 2 \times 2 = 8\)</span> choices, each with equal probability. In this example, the blue set corresponds to the event <span class="math inline">\(A = \{ x\in \{0,1\}^3 \;|\; x_0 = 0 \}\)</span> where the first coin toss is equal to <span class="math inline">\(0\)</span>, and the pink set corresponds to the event <span class="math inline">\(B = \{ x\in \{0,1\}^3 \;|\; x_1 = 1 \}\)</span> where the second coin toss is equal to <span class="math inline">\(1\)</span> (with their intersection having a purplish color). As we can see, each of these events contains <span class="math inline">\(4\)</span> elements (out of <span class="math inline">\(8\)</span> total) and so has probability <span class="math inline">\(1/2\)</span>. The intersection of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> contains two elements, and so the probability that both of these events occur is <span class="math inline">\(2/8 = 1/4\)</span>.</div>
</div>
<p>An <em>event</em> is simply a subset <span class="math inline">\(A\)</span> of <span class="math inline">\(\{0,1\}^n\)</span>.
The <em>probability of <span class="math inline">\(A\)</span></em>, denoted by <span class="math inline">\(\Pr_{x\sim \{0,1\}^n}[A]\)</span> (or <span class="math inline">\(\Pr[A]\)</span> for short, when the sample space is understood from the context), is the probability that an <span class="math inline">\(x\)</span> chosen uniformly at random will be contained in <span class="math inline">\(A\)</span>.
Note that this is the same as <span class="math inline">\(|A|/2^n\)</span> (where <span class="math inline">\(|A|\)</span> as usual denotes the number of elements in the set <span class="math inline">\(A\)</span>).
For example, the probability that <span class="math inline">\(x\)</span> has an even number of ones is <span class="math inline">\(\Pr[A]\)</span> where <span class="math inline">\(A=\{ x : \sum_{i=0}^{n-1} x_i \;= 0 \mod 2 \}\)</span>.
In the case <span class="math inline">\(n=3\)</span>, <span class="math inline">\(A=\{ 000,011,101,110 \}\)</span>, and hence <span class="math inline">\(\Pr[A]=\tfrac{4}{8}=\tfrac{1}{2}\)</span>
(see <a href="" class="ref">eventhreecoinsfig</a>).
It turns out this is true for every <span class="math inline">\(n\)</span>:</p>
<div class="float" id="eventhreecoinsfig">
<img src="../figure/even3coins.png" class="margin" alt="The event that if we toss three coins x_0,x_1,x_2 \in \{0,1\} then the sum of the x_i’s is even has probability 1/2 since it corresponds to exactly 4 out of the 8 possible strings of length 3." />
<div class="figcaption">The event that if we toss three coins <span class="math inline">\(x_0,x_1,x_2 \in \{0,1\}\)</span> then the sum of the <span class="math inline">\(x_i\)</span>’s is even has probability <span class="math inline">\(1/2\)</span> since it corresponds to exactly <span class="math inline">\(4\)</span> out of the <span class="math inline">\(8\)</span> possible strings of length <span class="math inline">\(3\)</span>.</div>
</div>
<blockquote>
<h3 id="evenprob" class="lemma"></h3>
<p>For every <span class="math inline">\(n&gt;0\)</span>, <span class="math display">\[\Pr_{x\sim \{0,1\}^n}[ \text{$\sum_{i=0}^{n-1} x_i$ is even }] = 1/2\]</span></p>
</blockquote>
<blockquote>
<h3 id="section-2" class="pause"></h3>
<p>To test your intuition on probability, try to stop here and prove the lemma on your own.</p>
</blockquote>
<div class="proof" data-ref="evenprob">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>We prove the lemma by induction on <span class="math inline">\(n\)</span>. For the case <span class="math inline">\(n=1\)</span> it is clear since <span class="math inline">\(x=0\)</span> is even and <span class="math inline">\(x=1\)</span> is odd, and hence the probability that <span class="math inline">\(x\in \{0,1\}\)</span> is even is <span class="math inline">\(1/2\)</span>.
Let <span class="math inline">\(n&gt;1\)</span>. We assume by induction that the lemma is true for <span class="math inline">\(n-1\)</span> and we will prove it for <span class="math inline">\(n\)</span>.
We split the set <span class="math inline">\(\{0,1\}^n\)</span> into four disjoint sets <span class="math inline">\(E_0,E_1,O_0,O_1\)</span>, where for <span class="math inline">\(b\in \{0,1\}\)</span>, <span class="math inline">\(E_b\)</span> is defined as the set of <span class="math inline">\(x\in \{0,1\}^n\)</span> such that <span class="math inline">\(x_0\cdots x_{n-2}\)</span> has even number of ones and <span class="math inline">\(x_{n-1}=b\)</span> and similarly <span class="math inline">\(O_b\)</span> is the set of <span class="math inline">\(x\in \{0,1\}^n\)</span> such that <span class="math inline">\(x_0 \cdots x_{n-2}\)</span> has odd number of ones and <span class="math inline">\(x_{n-1}=b\)</span>.
Since <span class="math inline">\(E_0\)</span> is obtained by simply extending <span class="math inline">\(n-1\)</span>-length string with even number of ones by the digit <span class="math inline">\(0\)</span>, the size of <span class="math inline">\(E_0\)</span> is simply the number of such <span class="math inline">\(n-1\)</span>-length strings which by the induction hypothesis is <span class="math inline">\(2^{n-1}/2 = 2^{n-2}\)</span>.
The same reasoning applies for <span class="math inline">\(E_1\)</span>, <span class="math inline">\(O_0\)</span>, and <span class="math inline">\(O_1\)</span>.
Hence each one of the four sets <span class="math inline">\(E_0,E_1,O_0,O_1\)</span> is of size <span class="math inline">\(2^{n-2}\)</span>.
Since <span class="math inline">\(x\in \{0,1\}^n\)</span> has an even number of ones if and only if <span class="math inline">\(x \in E_0 \cup O_1\)</span> (i.e., either the first <span class="math inline">\(n-1\)</span> coordinates sum up to an even number and the final coordinate is <span class="math inline">\(0\)</span> or the first <span class="math inline">\(n-1\)</span> coordinates sum up to an odd number and the final coordinate is <span class="math inline">\(1\)</span>), we get that the probability that <span class="math inline">\(x\)</span> satisfies this property is
<span class="math display">\[
\tfrac{|E_0\cup O_1|}{2^n} = \frac{2^{n-2}+2^{n-2}}{2^n} = \frac{1}{2} \;,
\]</span>
using the fact that <span class="math inline">\(E_0\)</span> and <span class="math inline">\(O_1\)</span> are disjoint and hence <span class="math inline">\(|E_0 \cup O_1| = |E_0|+|O_1|\)</span>.</p>
</div>
<p>We can also use the <em>intersection</em> (<span class="math inline">\(\cap\)</span>) and <em>union</em> (<span class="math inline">\(\cup\)</span>) operators to talk about the probability of both event <span class="math inline">\(A\)</span> <em>and</em> event <span class="math inline">\(B\)</span> happening, or the probability of event <span class="math inline">\(A\)</span> <em>or</em> event <span class="math inline">\(B\)</span> happening.
For example, the probability <span class="math inline">\(p\)</span> that <span class="math inline">\(x\)</span> has an <em>even</em> number of ones <em>and</em> <span class="math inline">\(x_0=1\)</span> is the same as
<span class="math inline">\(\Pr[A\cap B]\)</span> where <span class="math inline">\(A=\{ x\in \{0,1\}^n : \sum_{i=0}^{n-1} x_i =0 \mod 2 \}\)</span> and <span class="math inline">\(B=\{ x\in \{0,1\}^n : x_0 = 1 \}\)</span>.
This probability is equal to <span class="math inline">\(1/4\)</span> for <span class="math inline">\(n &gt; 1\)</span>. (It is a great exercise for you to pause here and verify that you understand why this is the case.)</p>
<p>Because intersection corresponds to considering the logical AND of the conditions that two events happen, while union corresponds to considering the logical OR, we will sometimes use the <span class="math inline">\(\wedge\)</span> and <span class="math inline">\(\vee\)</span> operators instead of <span class="math inline">\(\cap\)</span> and <span class="math inline">\(\cup\)</span>, and so write this probability <span class="math inline">\(p=\Pr[A \cap B]\)</span> defined above also as
<span class="math display">\[
\Pr_{x\sim \{0,1\}^n} \left[ \sum_i x_i =0 \mod 2 \; \wedge \; x_0 = 1 \right] \;.
\]</span></p>
<p>If <span class="math inline">\(A \subseteq \{0,1\}^n\)</span> is an event, then <span class="math inline">\(\overline{A} = \{0,1\}^n \setminus A\)</span> corresponds to the event that <span class="math inline">\(A\)</span> does <em>not</em> happen.
Since <span class="math inline">\(|\overline{A}|=2^n-|A|\)</span>, we get that
<span class="math display">\[\Pr[\overline{A}] = \tfrac{|\overline{A}|}{2^n} = \tfrac{2^n-|A|}{2^n}=1-\tfrac{|A|}{2^n} = 1- \Pr[A]
\]</span>
This makes sense: since <span class="math inline">\(A\)</span> happens if and only if <span class="math inline">\(\overline{A}\)</span> does <em>not</em> happen, the probability of <span class="math inline">\(\overline{A}\)</span> should be one minus the probability of <span class="math inline">\(A\)</span>.</p>
<blockquote>
<h3 id="samplespace" class="remark" title="Remember the sample space"></h3>
<p>While the above definition might seem very simple and almost trivial, the human mind seems not to have evolved for probabilistic reasoning, and it is surprising how often people can get even the simplest settings of probability wrong.
One way to make sure you don’t get confused when trying to calculate probability statements is to always ask yourself the following two questions: <strong>(1)</strong> Do I understand what is the <strong>sample space</strong> that this probability is taken over?, and <strong>(2)</strong> Do I understand what is the definition of the <strong>event</strong> that we are analyzing?.</p>
<p>For example, suppose that I were to randomize seating in my course, and then it turned out that students sitting in row 7 performed better on the final: how surprising should we find this? If we started out with the hypothesis that there is something special about the number 7 and chose it ahead of time, then the event that we are discussing is the event <span class="math inline">\(A\)</span> that students sitting in number 7 had better performance on the final, and we might find it surprising. However, if we first looked at the results and then chose the row whose average performance is best, then the event we are discussing is the event <span class="math inline">\(B\)</span> that there exists <em>some</em> row where the performance is higher than the overall average. <span class="math inline">\(B\)</span> is a superset of <span class="math inline">\(A\)</span>, and its probability (even if there is no correlation between sitting and performance) can be quite significant.</p>
</blockquote>
<div id="random-variables" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Random variables<a href="mathematical-background.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Events</em> correspond to Yes/No questions, but often we want to analyze finer questions.
For example, if we make a bet at the roulette wheel, we don’t want to just analyze whether we won or lost, but also <em>how much</em> we’ve gained.
A (real valued) <em>random variable</em> is simply a way to associate a number with the result of a probabilistic experiment.
Formally, a random variable is a function <span class="math inline">\(X:\{0,1\}^n \rightarrow \mathbb{R}\)</span> that maps every outcome <span class="math inline">\(x\in \{0,1\}^n\)</span> to an element <span class="math inline">\(X(x) \in \mathbb{R}\)</span>.
For example, the function <span class="math inline">\(sum:\{0,1\}^n \rightarrow \mathbb{R}\)</span> that maps <span class="math inline">\(x\)</span> to the sum of its coordinates (i.e., to <span class="math inline">\(\sum_{i=0}^{n-1} x_i\)</span>) is a random variable.</p>
<p>The <em>expectation</em> of a random variable <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(\mathbb{E}[X]\)</span>, is the average value that that this number takes, taken over all draws from the probabilistic experiment.
In other words, the expectation of <span class="math inline">\(X\)</span> is defined as follows:
<span class="math display">\[
\mathbb{E}[X] = \sum_{x\in \{0,1\}^n} 2^{-n}X(x) \;.
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables, then we can define <span class="math inline">\(X+Y\)</span> as simply the random variable that maps a point <span class="math inline">\(x\in \{0,1\}^n\)</span> to <span class="math inline">\(X(x)+Y(x)\)</span>.
One basic and very useful property of the expectation is that it is <em>linear</em>:</p>
<blockquote>
<h3 id="linearityexp" class="lemma" title="Linearity of expectation"></h3>
<p><span class="math display">\[ \mathbb{E}[ X+Y ] = \mathbb{E}[X] + \mathbb{E}[Y] \]</span></p>
</blockquote>
<blockquote>
<h3 id="section-3" class="proof"></h3>
<p><span class="math display">\[
\begin{gathered}
\mathbb{E}[X+Y] = \sum_{x\in \{0,1\}^n}2^{-n}\left(X(x)+Y(x)\right) =  \\
\sum_{x\in \{0,1\}^b} 2^{-n}X(x) + \sum_{x\in \{0,1\}^b} 2^{-n}Y(x) = \\
\mathbb{E}[X] + \mathbb{E}[Y]
\end{gathered}
\]</span></p>
</blockquote>
<p>Similarly, <span class="math inline">\(\mathbb{E}[kX] = k\mathbb{E}[X]\)</span> for every <span class="math inline">\(k \in \mathbb{R}\)</span>.
For example, using the linearity of expectation, it is very easy to show that the expectation of the sum of the <span class="math inline">\(x_i\)</span>’s for <span class="math inline">\(x \sim \{0,1\}^n\)</span> is equal to <span class="math inline">\(n/2\)</span>.
Indeed, if we write <span class="math inline">\(X= \sum_{i=0}^{n-1} x_i\)</span> then <span class="math inline">\(X= X_0 + \cdots + X_{n-1}\)</span> where <span class="math inline">\(X_i\)</span> is the random variable <span class="math inline">\(x_i\)</span>. Since for every <span class="math inline">\(i\)</span>, <span class="math inline">\(\Pr[X_i=0] = 1/2\)</span> and <span class="math inline">\(\Pr[X_i=1]=1/2\)</span>, we get that <span class="math inline">\(\mathbb{E}[X_i] = (1/2)\cdot 0 + (1/2)\cdot 1 = 1/2\)</span> and hence <span class="math inline">\(\mathbb{E}[X] = \sum_{i=0}^{n-1}\mathbb{E}[X_i] = n\cdot(1/2) = n/2\)</span>.</p>
<blockquote>
<h3 id="section-4" class="pause"></h3>
<p>If you have not seen discrete probability before, please go over this argument again until you are sure you follow it; it is a prototypical simple example of the type of reasoning we will employ again and again in this course.</p>
</blockquote>
<p>If <span class="math inline">\(A\)</span> is an event, then <span class="math inline">\(1_A\)</span> is the random variable such that <span class="math inline">\(1_A(x)\)</span> equals <span class="math inline">\(1\)</span> if <span class="math inline">\(x\in A\)</span>, and <span class="math inline">\(1_A(x)=0\)</span> otherwise.
Note that <span class="math inline">\(\Pr[A] = \mathbb{E}[1_A]\)</span> (can you see why?).
Using this and the linearity of expectation, we can show one of the most useful bounds in probability theory:</p>
<blockquote>
<h3 id="unionbound" class="lemma" title="Union bound"></h3>
<p>For every two events <span class="math inline">\(A,B\)</span>, <span class="math inline">\(\Pr[ A \cup B] \leq \Pr[A]+\Pr[B]\)</span></p>
</blockquote>
<blockquote>
<h3 id="section-5" class="pause"></h3>
<p>Before looking at the proof, try to see why the union bound makes intuitive sense. We can also prove it directly from the definition of probabilities and the cardinality of sets, together with the equation <span class="math inline">\(|A \cup B| \leq |A|+|B|\)</span>. Can you see why the latter equation is true? (See also <a href="" class="ref">unionboundfig</a>.)</p>
</blockquote>
<blockquote>
<h3 id="section-6" class="proof"></h3>
<p>For every <span class="math inline">\(x\)</span>, the variable <span class="math inline">\(1_{A\cup B}(x) \leq 1_A(x)+1_B(x)\)</span>.
Hence, <span class="math inline">\(\Pr[A\cup B] = \mathbb{E}[ 1_{A \cup B} ] \leq \mathbb{E}[1_A+1_B] = \mathbb{E}[1_A]+\mathbb{E}[1_B] = \Pr[A]+\Pr[B]\)</span>.
&amp; &amp;</p>
</blockquote>
<p>The way we often use this in theoretical computer science is to argue that, for example, if there is a list of 100 bad events that can happen, and each one of them happens with probability at most <span class="math inline">\(1/10000\)</span>, then with probability at least <span class="math inline">\(1-100/10000 = 0.99\)</span>, no bad event happens.</p>
<div class="float" id="unionboundfig">
<img src="../figure/unionbound.png" class="margin" alt="The union bound tells us that the probability of A or B happening is at most the sum of the individual probabilities. We can see it by noting that for every two sets |A\cup B| \leq |A|+|B| (with equality only if A and B have no intersection)." />
<div class="figcaption">The <em>union bound</em> tells us that the probability of <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> happening is at most the sum of the individual probabilities. We can see it by noting that for every two sets <span class="math inline">\(|A\cup B| \leq |A|+|B|\)</span> (with equality only if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have no intersection).</div>
</div>
</div>
<div id="distributions-over-strings" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Distributions over strings<a href="mathematical-background.html#distributions-over-strings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While most of the time we think of random variables as having as output a <em>real number</em>, we sometimes consider random variables whose output is a <em>string</em>.
That is, we can think of a map <span class="math inline">\(Y:\{0,1\}^n \rightarrow \{0,1\}^*\)</span> and consider the “random variable” <span class="math inline">\(Y\)</span> such that for every <span class="math inline">\(y\in \{0,1\}^*\)</span>, the probability that <span class="math inline">\(Y\)</span> outputs <span class="math inline">\(y\)</span> is equal to <span class="math inline">\(\tfrac{1}{2^n}\left| \{ x \in \{0,1\}^n \;|\; Y(x)=y \}\right|\)</span>.
To avoid confusion, we will typically refer to such string-valued random variables as <em>distributions</em> over strings.
So, a <em>distribution</em> <span class="math inline">\(Y\)</span> over strings <span class="math inline">\(\{0,1\}^*\)</span> can be thought of as a finite collection of strings <span class="math inline">\(y_0,\ldots,y_{M-1} \in \{0,1\}^*\)</span> and probabilities <span class="math inline">\(p_0,\ldots,p_{M-1}\)</span> (which are non-negative numbers summing up to one), so that <span class="math inline">\(\Pr[ Y = y_i ] = p_i\)</span>.</p>
<p>Two distributions <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y&#39;\)</span> are <em>identical</em> if they assign the same probability to every string.
For example, consider the following two functions <span class="math inline">\(Y,Y&#39;:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span>.
For every <span class="math inline">\(x \in \{0,1\}^2\)</span>, we define <span class="math inline">\(Y(x)=x\)</span> and <span class="math inline">\(Y&#39;(x)=x_0(x_0\oplus x_1)\)</span> where <span class="math inline">\(\oplus\)</span> is the XOR operations.
Although these are two different functions, they induce the same distribution over <span class="math inline">\(\{0,1\}^2\)</span> when invoked on a uniform input.
The distribution <span class="math inline">\(Y(x)\)</span> for <span class="math inline">\(x\sim \{0,1\}^2\)</span> is of course the uniform distribution over <span class="math inline">\(\{0,1\}^2\)</span>.
On the other hand <span class="math inline">\(Y&#39;\)</span> is simply the map <span class="math inline">\(00 \mapsto 00\)</span>, <span class="math inline">\(01 \mapsto 01\)</span>, <span class="math inline">\(10 \mapsto 11\)</span>, <span class="math inline">\(11 \mapsto 10\)</span> which is a permutation over
the map <span class="math inline">\(F:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span> defined as <span class="math inline">\(F(x_0x_1)=x_0x_1\)</span> and the map <span class="math inline">\(G:\{0,1\}^2 \rightarrow \{0,1\}^2\)</span> defined as <span class="math inline">\(G(x_0x_1)=x_0(x_0 \oplus x_1)\)</span></p>
</div>
<div id="more-general-sample-spaces." class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> More general sample spaces.<a href="mathematical-background.html#more-general-sample-spaces." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While in this chapter we assume that the underlying probabilistic experiment corresponds to tossing <span class="math inline">\(n\)</span> independent coins, everything we say easily generalizes to sampling <span class="math inline">\(x\)</span> from a more general finite or countable set <span class="math inline">\(S\)</span> (and not-so-easily generalizes to uncountable sets <span class="math inline">\(S\)</span> as well).
A <em>probability distribution</em> over a finite set <span class="math inline">\(S\)</span> is simply a function <span class="math inline">\(\mu : S \rightarrow [0,1]\)</span> such that
<span class="math inline">\(\sum_{x\in S}\mu(s)=1\)</span>.
We think of this as the experiment where we obtain every <span class="math inline">\(x\in S\)</span> with probability <span class="math inline">\(\mu(s)\)</span>, and sometimes denote this as <span class="math inline">\(x\sim \mu\)</span>.
An <em>event</em> <span class="math inline">\(A\)</span> is a subset of <span class="math inline">\(S\)</span>, and the probability of <span class="math inline">\(A\)</span>, which we denote by <span class="math inline">\(\Pr_\mu[A]\)</span>, is <span class="math inline">\(\sum_{x\in A} \mu(x)\)</span>.
A <em>random variable</em> is a function <span class="math inline">\(X:S \rightarrow \mathbb{R}\)</span>, where the probability that <span class="math inline">\(X=y\)</span> is equal to <span class="math inline">\(\sum_{x\in S \text{ s.t. } X(x)=y} \mu(x)\)</span>.</p>
</div>
</div>
<div id="correlations-and-independence" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Correlations and independence<a href="mathematical-background.html#correlations-and-independence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the most delicate but important concepts in probability is the notion of <em>independence</em> (and the opposing notion of <em>correlations</em>).
Subtle correlations are often behind surprises and errors in probability and statistical analysis, and several mistaken predictions have been blamed on miscalculating the correlations between, say, housing prices in Florida and Arizona, or voter preferences in Ohio and Michigan. See also Joe Blitzstein’s aptly named talk <a href="https://youtu.be/dzFf3r1yph8">“Conditioning is the Soul of Statistics”</a>. (Another thorny issue is of course the difference between <em>correlation</em> and <em>causation</em>. Luckily, this is another point we don’t need to worry about in our clean setting of tossing <span class="math inline">\(n\)</span> coins.)</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> if the fact that <span class="math inline">\(A\)</span> happens makes <span class="math inline">\(B\)</span> neither more nor less likely to happen.
For example, if we think of the experiment of tossing <span class="math inline">\(3\)</span> random coins <span class="math inline">\(x\in \{0,1\}^3\)</span>, and we let <span class="math inline">\(A\)</span> be the event that <span class="math inline">\(x_0=1\)</span> and <span class="math inline">\(B\)</span> the event that <span class="math inline">\(x_0 + x_1 + x_2 \geq 2\)</span>, then if <span class="math inline">\(A\)</span> happens it is more likely that <span class="math inline">\(B\)</span> happens, and hence these events are <em>not</em> independent.
On the other hand, if we let <span class="math inline">\(C\)</span> be the event that <span class="math inline">\(x_1=1\)</span>, then because the second coin toss is not affected by the result of the first one, the events <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> are independent.</p>
<p>The formal definition is that events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> if <span class="math inline">\(\Pr[A \cap B]=\Pr[A] \cdot \Pr[B]\)</span>.
If <span class="math inline">\(\Pr[A \cap B] &gt; \Pr[A]\cdot \Pr[B]\)</span> then we say that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>positively correlated</em>, while if <span class="math inline">\(\Pr[ A \cap B] &lt; \Pr[A] \cdot \Pr[B]\)</span> then we say that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>negatively correlated</em> (see <a href="" class="ref">coinexperimentfig</a>).</p>
<div class="float" id="independencefig">
<img src="../figure/independence.png" class="margin" alt="Two events A and B are independent if \Pr[A \cap B]=\Pr[A]\cdot \Pr[B]. In the two figures above, the empty x\times x square is the sample space, and A and B are two events in this sample space. In the left figure, A and B are independent, while in the right figure they are negatively correlated, since B is less likely to occur if we condition on A (and vice versa). Mathematically, one can see this by noticing that in the left figure the areas of A and B respectively are a\cdot x and b\cdot x, and so their probabilities are \tfrac{a\cdot x}{x^2}=\tfrac{a}{x} and \tfrac{b\cdot x}{x^2}=\tfrac{b}{x} respectively, while the area of A \cap B is a\cdot b which corresponds to the probability \tfrac{a\cdot b}{x^2}. In the right figure, the area of the triangle B is \tfrac{b\cdot x}{2} which corresponds to a probability of \tfrac{b}{2x}, but the area of A \cap B is \tfrac{b&#39; \cdot a}{2} for some b&#39;&lt;b. This means that the probability of A \cap B is \tfrac{b&#39;\cdot a}{2x^2} &lt; \tfrac{b}{2x} \cdot \tfrac{a}{x}, or in other words \Pr[A \cap B ] &lt; \Pr[A] \cdot \Pr[B]." />
<div class="figcaption">Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> if <span class="math inline">\(\Pr[A \cap B]=\Pr[A]\cdot \Pr[B]\)</span>. In the two figures above, the empty <span class="math inline">\(x\times x\)</span> square is the sample space, and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are two events in this sample space. In the left figure, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, while in the right figure they are negatively correlated, since <span class="math inline">\(B\)</span> is less likely to occur if we condition on <span class="math inline">\(A\)</span> (and vice versa). Mathematically, one can see this by noticing that in the left figure the areas of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> respectively are <span class="math inline">\(a\cdot x\)</span> and <span class="math inline">\(b\cdot x\)</span>, and so their probabilities are <span class="math inline">\(\tfrac{a\cdot x}{x^2}=\tfrac{a}{x}\)</span> and
<span class="math inline">\(\tfrac{b\cdot x}{x^2}=\tfrac{b}{x}\)</span> respectively, while the area of <span class="math inline">\(A \cap B\)</span> is <span class="math inline">\(a\cdot b\)</span> which corresponds to the probability <span class="math inline">\(\tfrac{a\cdot b}{x^2}\)</span>. In the right figure, the area of the triangle <span class="math inline">\(B\)</span> is <span class="math inline">\(\tfrac{b\cdot x}{2}\)</span> which corresponds to a probability of <span class="math inline">\(\tfrac{b}{2x}\)</span>, but the area of <span class="math inline">\(A \cap B\)</span> is <span class="math inline">\(\tfrac{b&#39; \cdot a}{2}\)</span> for some <span class="math inline">\(b&#39;&lt;b\)</span>. This means that the probability of <span class="math inline">\(A \cap B\)</span> is <span class="math inline">\(\tfrac{b&#39;\cdot a}{2x^2} &lt; \tfrac{b}{2x} \cdot \tfrac{a}{x}\)</span>, or in other words <span class="math inline">\(\Pr[A \cap B ] &lt; \Pr[A] \cdot \Pr[B]\)</span>.</div>
</div>
<p>If we consider the above examples on the experiment of choosing <span class="math inline">\(x\in \{0,1\}^3\)</span> then we can see that</p>
<p><span class="math display">\[
\begin{aligned}
\Pr[x_0=1] &amp;= \tfrac{1}{2} \\
\Pr[x_0+x_1+x_2 \geq 2] = \Pr[\{ 011,101,110,111 \}] &amp;= \tfrac{4}{8} = \tfrac{1}{2}
\end{aligned}
\]</span></p>
<p>but</p>
<p><span class="math display">\[
\Pr[x_0 =1 \; \wedge \; x_0+x_1+x_2 \geq 2 ] = \Pr[ \{101,110,111 \} ] = \tfrac{3}{8} &gt; \tfrac{1}{2} \cdot \tfrac{1}{2}
\]</span></p>
<p>and hence, as we already observed, the events <span class="math inline">\(\{ x_0 = 1 \}\)</span> and <span class="math inline">\(\{ x_0+x_1+x_2 \geq 2 \}\)</span> are not independent and in fact are positively correlated.
On the other hand, <span class="math inline">\(\Pr[ x_0 = 1 \wedge x_1 = 1 ] = \Pr[ \{110,111 \}] = \tfrac{2}{8} = \tfrac{1}{2} \cdot \tfrac{1}{2}\)</span> and hence the events <span class="math inline">\(\{x_0 = 1 \}\)</span> and <span class="math inline">\(\{ x_1 = 1 \}\)</span> are indeed independent.</p>
<blockquote>
<h3 id="disjoint" class="remark" title="Disjointness vs independence"></h3>
<p>People sometimes confuse the notion of <em>disjointness</em> and <em>independence</em>, but these are actually quite different.
Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>disjoint</em> if <span class="math inline">\(A \cap B = \emptyset\)</span>, which means that if <span class="math inline">\(A\)</span> happens then <span class="math inline">\(B\)</span> definitely does not happen. They are <em>independent</em> if <span class="math inline">\(\Pr[A \cap B]=\Pr[A]\Pr[B]\)</span> which means that knowing that <span class="math inline">\(A\)</span> happens gives us no information about whether <span class="math inline">\(B\)</span> happened or not. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have nonzero probability, then being disjoint implies that they are <em>not</em> independent, since in particular it means that they are negatively correlated.</p>
</blockquote>
<p><strong>Conditional probability:</strong> If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events, and <span class="math inline">\(A\)</span> happens with nonzero probability then we define the probability that <span class="math inline">\(B\)</span> happens <em>conditioned on <span class="math inline">\(A\)</span></em> to be <span class="math inline">\(\Pr[B|A] = \Pr[A \cap B]/\Pr[A]\)</span>.
This corresponds to calculating the probability that <span class="math inline">\(B\)</span> happens if we already know that <span class="math inline">\(A\)</span> happened.
Note that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if and only if <span class="math inline">\(\Pr[B|A]=\Pr[B]\)</span>.</p>
<p><strong>More than two events:</strong> We can generalize this definition to more than two events.
We say that events <span class="math inline">\(A_1,\ldots,A_k\)</span> are <em>mutually independent</em> if knowing that any set of them occurred or didn’t occur does not change the probability that an event outside the set occurs.
Formally, the condition is that for every subset <span class="math inline">\(I \subseteq [k]\)</span>,
<span class="math display">\[
\Pr[ \wedge_{i\in I} A_i] =\prod_{i\in I} \Pr[A_i].
\]</span></p>
<p>For example, if <span class="math inline">\(x\sim \{0,1\}^3\)</span>, then the events <span class="math inline">\(\{ x_0=1 \}\)</span>, <span class="math inline">\(\{ x_1 = 1\}\)</span> and <span class="math inline">\(\{x_2 = 1 \}\)</span> are mutually independent.
On the other hand, the events <span class="math inline">\(\{x_0 = 1 \}\)</span>, <span class="math inline">\(\{x_1 = 1\}\)</span> and <span class="math inline">\(\{ x_0 + x_1 = 0 \mod 2 \}\)</span> are <em>not</em> mutually independent, even though every pair of these events is independent (can you see why? see also <a href="" class="ref">independencecoinsfig</a>).</p>
<div class="float" id="independencecoinsfig">
<img src="../figure/independencecoins.png" class="margin" alt="Consider the sample space \{0,1\}^n and the events A,B,C,D,E corresponding to A: x_0=1, B: x_1=1, C: x_0+x_1+x_2 \geq 2, D: x_0+x_1+x_2 = 0 mod 2 and D: x_0+x_1 = 0 mod 2. We can see that A and B are independent, C is positively correlated with A and positively correlated with B, the three events A,B,D are mutually independent, and while every pair out of A,B,E is independent, the three events A,B,E are not mutually independent since their intersection has probability \tfrac{2}{8}=\tfrac{1}{4} instead of \tfrac{1}{2}\cdot \tfrac{1}{2} \cdot \tfrac{1}{2} = \tfrac{1}{8}." />
<div class="figcaption">Consider the sample space <span class="math inline">\(\{0,1\}^n\)</span> and the events <span class="math inline">\(A,B,C,D,E\)</span> corresponding to <span class="math inline">\(A\)</span>: <span class="math inline">\(x_0=1\)</span>, <span class="math inline">\(B\)</span>: <span class="math inline">\(x_1=1\)</span>, <span class="math inline">\(C\)</span>: <span class="math inline">\(x_0+x_1+x_2 \geq 2\)</span>, <span class="math inline">\(D\)</span>: <span class="math inline">\(x_0+x_1+x_2 = 0 mod 2\)</span> and <span class="math inline">\(D\)</span>: <span class="math inline">\(x_0+x_1 = 0 mod 2\)</span>. We can see that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, <span class="math inline">\(C\)</span> is positively correlated with <span class="math inline">\(A\)</span> and positively correlated with <span class="math inline">\(B\)</span>, the three events <span class="math inline">\(A,B,D\)</span> are mutually independent, and while every pair out of <span class="math inline">\(A,B,E\)</span> is independent, the three events <span class="math inline">\(A,B,E\)</span> are not mutually independent since their intersection has probability <span class="math inline">\(\tfrac{2}{8}=\tfrac{1}{4}\)</span> instead of <span class="math inline">\(\tfrac{1}{2}\cdot \tfrac{1}{2} \cdot \tfrac{1}{2} = \tfrac{1}{8}\)</span>.</div>
</div>
<div id="independent-random-variables" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Independent random variables<a href="mathematical-background.html#independent-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We say that two random variables <span class="math inline">\(X:\{0,1\}^n \rightarrow \mathbb{R}\)</span> and <span class="math inline">\(Y:\{0,1\}^n \rightarrow \mathbb{R}\)</span> are independent if for every <span class="math inline">\(u,v \in \mathbb{R}\)</span>, the events <span class="math inline">\(\{ X=u \}\)</span> and <span class="math inline">\(\{ Y=v \}\)</span> are independent. (We use <span class="math inline">\(\{ X=u \}\)</span> as shorthand for <span class="math inline">\(\{ x \;|\; X(x)=u \}\)</span>.)
In other words, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if <span class="math inline">\(\Pr[ X=u \wedge Y=v]=\Pr[X=u]\Pr[Y=v]\)</span> for every <span class="math inline">\(u,v \in \mathbb{R}\)</span>.
For example, if two random variables depend on the result of tossing different coins then they are independent:</p>
<blockquote>
<h3 id="indcoins" class="lemma"></h3>
<p>Suppose that <span class="math inline">\(S=\{ s_0,\ldots, s_{k-1} \}\)</span> and <span class="math inline">\(T=\{ t_0 ,\ldots, t_{m-1} \}\)</span> are disjoint subsets of <span class="math inline">\(\{0,\ldots,n-1\}\)</span> and let
<span class="math inline">\(X,Y:\{0,1\}^n \rightarrow \mathbb{R}\)</span> be random variables such that <span class="math inline">\(X=F(x_{s_0},\ldots,x_{s_{k-1}})\)</span> and <span class="math inline">\(Y=G(x_{t_0},\ldots,x_{t_{m-1}})\)</span> for some functions <span class="math inline">\(F: \{0,1\}^k \rightarrow \mathbb{R}\)</span> and <span class="math inline">\(G: \{0,1\}^m \rightarrow \mathbb{R}\)</span>.
Then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>
</blockquote>
<blockquote>
<h3 id="section-7" class="pause"></h3>
<p>The notation in the lemma’s statement is a bit cumbersome, but at the end of the day, it simply says that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables that depend on two disjoint sets <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> of coins (for example, <span class="math inline">\(X\)</span> might be the sum of the first <span class="math inline">\(n/2\)</span> coins, and <span class="math inline">\(Y\)</span> might be the largest consecutive stretch of zeroes in the second <span class="math inline">\(n/2\)</span> coins), then they are independent.</p>
</blockquote>
<blockquote>
<h3 id="section-8" class="proof"></h3>
<p>Let <span class="math inline">\(a,b\in \mathbb{R}\)</span>, and let <span class="math inline">\(A = \{ x \in \{0,1\}^k : F(x)=a \}\)</span> and <span class="math inline">\(B=\{ x\in \{0,1\}^m : G(x)=b \}\)</span>.
Since <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> are disjoint, we can reorder the indices so that <span class="math inline">\(S = \{0,\ldots,k-1\}\)</span> and <span class="math inline">\(T=\{k,\ldots,k+m-1\}\)</span> without affecting any of the probabilities.
Hence we can write <span class="math inline">\(\Pr[X=a \wedge Y=b] = |C|/2^n\)</span> where <span class="math inline">\(C= \{ x_0,\ldots,x_{n-1} : (x_0,\ldots,x_{k-1}) \in A \wedge (x_k,\ldots,x_{k+m-1}) \in B \}\)</span>.
Another way to write this using string concatenation is that <span class="math inline">\(C = \{ xyz : x\in A, y\in B, z\in \{0,1\}^{n-k-m} \}\)</span>, and hence <span class="math inline">\(|C|=|A||B|2^{n-k-m}\)</span>, which means that
<span class="math display">\[
\tfrac{|C|}{2^n} = \tfrac{|A|}{2^k}\tfrac{|B|}{2^m}\tfrac{2^{n-k-m}}{2^{n-k-m}}=\Pr[X=a]\Pr[Y=b] .
\]</span></p>
</blockquote>
<p>Note that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables then (if we let <span class="math inline">\(S_X,S_Y\)</span> denote all the numbers that have positive probability of being the output of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively) it holds that:
<span class="math display">\[
\begin{gathered}
\mathbb{E}[ XY ] = \sum_{a \in S_X,b \in S_Y} {\textstyle\Pr[X=a \wedge Y=b]}\cdot ab \; =^{(1)} \; \sum_{a \in S_X,b \in S_Y} {\textstyle \Pr[X=a]\Pr[Y=b]}\cdot ab =^{(2)} \\
\left(\sum_{a \in S_X} {\textstyle \Pr[X=a]}\cdot a\right)\left(\sum_{b \in S_Y} {\textstyle \Pr[Y=b]}b\right) =^{(3)} \\
\mathbb{E}[X] \mathbb{E}[Y]
\end{gathered}
\]</span>
where the first equality (<span class="math inline">\(=^{(1)}\)</span>) follows from the independence of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the second equality (<span class="math inline">\(=^{(2)}\)</span>) follows by “opening the parentheses” of the righthand side, and the third inequality (<span class="math inline">\(=^{(3)}\)</span>) follows from the definition of expectation.
(This is not an “if and only if”; see <a href="" class="ref">noindnocorex</a>.)</p>
<p>Another useful fact is that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then so are <span class="math inline">\(F(X)\)</span> and <span class="math inline">\(G(Y)\)</span> for all functions <span class="math inline">\(F,G:\mathbb{R}\rightarrow R\)</span>.
This is intuitively true since learning <span class="math inline">\(F(X)\)</span> can only provide us with less information than does learning <span class="math inline">\(X\)</span> itself.
Hence, if learning <span class="math inline">\(X\)</span> does not teach us anything about <span class="math inline">\(Y\)</span> (and so also about <span class="math inline">\(F(Y)\)</span>) then neither will learning <span class="math inline">\(F(X)\)</span>.
Indeed, to prove this we can write for every <span class="math inline">\(a,b \in \mathbb{R}\)</span>:</p>
<p><span class="math display">\[
\begin{gathered}
\Pr[ F(X)=a \wedge G(Y)=b ] = \sum_{x \text{ s.t.} F(x)=a, y \text{ s.t. } G(y)=b} \Pr[ X=x \wedge Y=y ] = \\
\sum_{x \text{ s.t.} F(x)=a, y \text{ s.t. } G(y)=b} \Pr[ X=x ] \Pr[  Y=y ]  = \\
\left( \sum_{x \text{ s.t.} F(x)=a } \Pr[X=x ] \right) \cdot \left( \sum_{y \text{ s.t.} G(y)=b } \Pr[Y=y ] \right) = \\
\Pr[ F(X)=a] \Pr[G(Y)=b] .
\end{gathered}
\]</span></p>
</div>
<div id="collections-of-independent-random-variables." class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Collections of independent random variables.<a href="mathematical-background.html#collections-of-independent-random-variables." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can extend the notions of independence to more than two random variables:
we say that the random variables <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are <em>mutually independent</em> if for every <span class="math inline">\(a_0,\ldots,a_{n-1} \in \mathbb{R}\)</span>,
<span class="math display">\[
\Pr\left[X_0=a_0 \wedge \cdots \wedge X_{n-1}=a_{n-1}\right]=\Pr[X_0=a_0]\cdots \Pr[X_{n-1}=a_{n-1}] .
\]</span>
And similarly, we have that</p>
<blockquote>
<h3 id="expprod" class="lemma" title="Expectation of product of independent random variables"></h3>
<p>If <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are mutually independent then
<span class="math display">\[
\mathbb{E}[ \prod_{i=0}^{n-1} X_i ] = \prod_{i=0}^{n-1} \mathbb{E}[X_i] .
\]</span></p>
</blockquote>
<blockquote>
<h3 id="indeplem" class="lemma" title="Functions preserve independence"></h3>
<p>If <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are mutually independent, and <span class="math inline">\(Y_0,\ldots,Y_{n-1}\)</span> are defined as <span class="math inline">\(Y_i = F_i(X_i)\)</span> for some functions <span class="math inline">\(F_0,\ldots,F_{n-1}:\mathbb{R}\rightarrow \mathbb{R}\)</span>, then <span class="math inline">\(Y_0,\ldots,Y_{n-1}\)</span> are mutually independent as well.</p>
</blockquote>
<blockquote>
<h3 id="section-9" class="pause"></h3>
<p>We leave proving <a href="" class="ref">expprod</a> and <a href="" class="ref">indeplem</a> as <a href="" class="ref">expprodex</a> <a href="" class="ref">indeplemex</a>.
It is good idea for you stop now and do these exercises to make sure you are comfortable with the notion of independence, as we will use it heavily later on in this course.</p>
</blockquote>
</div>
</div>
<div id="concentration-and-tail-bounds" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Concentration and tail bounds<a href="mathematical-background.html#concentration-and-tail-bounds" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The name “expectation” is somewhat misleading.
For example, suppose that you and I place a bet on the outcome of 10 coin tosses, where if they all come out to be <span class="math inline">\(1\)</span>’s then I pay you 100,000 dollars and otherwise you pay me 10 dollars.
If we let <span class="math inline">\(X:\{0,1\}^{10} \rightarrow \mathbb{R}\)</span> be the random variable denoting your gain, then we see that</p>
<p><span class="math display">\[
\mathbb{E}[X] = 2^{-10}\cdot 100000 - (1-2^{-10})10 \sim 90 .
\]</span></p>
<p>But we don’t really “expect” the result of this experiment to be for you to gain 90 dollars.
Rather, 99.9% of the time you will pay me 10 dollars, and you will hit the jackpot 0.01% of the times.</p>
<p>However, if we repeat this experiment again and again (with fresh and hence <em>independent</em> coins), then in the long run we do expect your average earning to be close to 90 dollars, which is the reason why casinos can make money in a predictable way even though every individual bet is random.
For example, if we toss <span class="math inline">\(n\)</span> independent and unbiased coins, then as <span class="math inline">\(n\)</span> grows, the number of coins that come up ones will be more and more <em>concentrated</em> around <span class="math inline">\(n/2\)</span> according to the famous “bell curve” (see <a href="" class="ref">bellfig</a>).</p>
<div class="float" id="bellfig">
<img src="../figure/binomial.png" class="margin" alt="The probabilities that we obtain a particular sum when we toss n=10,20,100,1000 coins converge quickly to the Gaussian/normal distribution." />
<div class="figcaption">The probabilities that we obtain a particular sum when we toss <span class="math inline">\(n=10,20,100,1000\)</span> coins converge quickly to the Gaussian/normal distribution.</div>
</div>
<p>Much of probability theory is concerned with so called <em>concentration</em> or <em>tail</em> bounds, which are upper bounds on the probability that a random variable <span class="math inline">\(X\)</span> deviates too much from its expectation.
The first and simplest one of them is Markov’s inequality:</p>
<blockquote>
<h3 id="markovthm" class="theorem" title="Markov&#39;s inequality"></h3>
<p>If <span class="math inline">\(X\)</span> is a non-negative random variable then <span class="math inline">\(\Pr[ X \geq k \mathbb{E}[X] ] \leq 1/k\)</span>.</p>
</blockquote>
<blockquote>
<h3 id="section-10" class="pause"></h3>
<p>Markov’s Inequality is actually a very natural statement (see also <a href="" class="ref">markovfig</a>). For example, if you know that the average (not the median!) household income in the US is 70,000 dollars, then in particular you can deduce that at most 25 percent of households make more than 280,000 dollars, since otherwise, even if the remaining 75 percent had zero income, the top 25 percent alone would cause the average income to be larger than 70,000 dollars. From this example you can already see that in many situations, Markov’s inequality will not be <em>tight</em> and the probability of deviating from expectation will be much smaller: see the Chebyshev and Chernoff inequalities below.</p>
</blockquote>
<blockquote>
<h3 id="section-11" class="proof"></h3>
<p>Let <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> and define <span class="math inline">\(Y=1_{X \geq k \mu}\)</span>. That is, <span class="math inline">\(Y(x)=1\)</span> if <span class="math inline">\(X(x) \geq k \mu\)</span> and <span class="math inline">\(Y(x)=0\)</span> otherwise.
Note that by definition, for every <span class="math inline">\(x\)</span>, <span class="math inline">\(Y(x) \leq X/(k\mu)\)</span>.
We need to show <span class="math inline">\(\mathbb{E}[Y] \leq 1/k\)</span>.
But this follows since <span class="math inline">\(\mathbb{E}[Y] \leq \mathbb{E}[X/k(\mu)] = \mathbb{E}[X]/(k\mu) = \mu/(k\mu)=1/k\)</span>.</p>
</blockquote>
<div class="float" id="markovfig">
<img src="../figure/markovineq.png" class="margin" alt="Markov’s Inequality tells us that a non-negative random variable X cannot be much larger than its expectation, with high probability. For example, if the expectation of X is \mu, then the probability that X&gt;4\mu must be at most 1/4, as otherwise just the contribution from this part of the sample space will be too large." />
<div class="figcaption">Markov’s Inequality tells us that a non-negative random variable <span class="math inline">\(X\)</span> cannot be much larger than its expectation, with high probability. For example, if the expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(\mu\)</span>, then the probability that <span class="math inline">\(X&gt;4\mu\)</span> must be at most <span class="math inline">\(1/4\)</span>, as otherwise just the contribution from this part of the sample space will be too large.</div>
</div>
<p><strong>Going beyond Markov’s Inequality:</strong>
Markov’s inequality says that a (non-negative) random variable <span class="math inline">\(X\)</span> can’t go too crazy and be, say, a million times its expectation, with significant probability.
But ideally we would like to say that with high probability, <span class="math inline">\(X\)</span> should be very close to its expectation, e.g., in the range <span class="math inline">\([0.99 \mu, 1.01 \mu]\)</span> where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>.
This is not generally true, but does turn out to hold when <span class="math inline">\(X\)</span> is obtained by combining (e.g., adding) many independent random variables.
This phenomenon, variants of which are known as “law of large numbers”, “central limit theorem”, “invariance principles” and “Chernoff bounds”, is one of the most fundamental in probability and statistics, and is one that we heavily use in computer science as well.</p>
<div id="chebyshevs-inequality" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Chebyshev’s Inequality<a href="mathematical-background.html#chebyshevs-inequality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A standard way to measure the deviation of a random variable from its expectation is by using its <em>standard deviation</em>.
For a random variable <span class="math inline">\(X\)</span>, we define the <em>variance</em> of <span class="math inline">\(X\)</span> as <span class="math inline">\(\mathrm{Var}[X] = \mathbb{E}[(X-\mu)^2]\)</span> where <span class="math inline">\(\mu = \mathbb{E}[X]\)</span>; i.e., the variance is the average squared distance of <span class="math inline">\(X\)</span> from its expectation.
The <em>standard deviation</em> of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(\sigma[X] = \sqrt{\mathrm{Var}[X]}\)</span>.
(This is well-defined since the variance, being an average of a square, is always a non-negative number.)</p>
<p>Using Chebyshev’s inequality, we can control the probability that a random variable is too many standard deviations away from its expectation.</p>
<blockquote>
<h3 id="chebychevthm" class="theorem" title="Chebyshev&#39;s inequality"></h3>
<p>Suppose that <span class="math inline">\(\mu=\mathbb{E}[X]\)</span> and <span class="math inline">\(\sigma^2 = \mathrm{Var}[X]\)</span>.
Then for every <span class="math inline">\(k&gt;0\)</span>, <span class="math inline">\(\Pr[ |X-\mu | \geq k \sigma ] \leq 1/k^2\)</span>.</p>
</blockquote>
<blockquote>
<h3 id="section-12" class="proof"></h3>
<p>The proof follows from Markov’s inequality.
We define the random variable <span class="math inline">\(Y = (X-\mu)^2\)</span>.
Then <span class="math inline">\(\mathbb{E}[Y] = \mathrm{Var}[X] = \sigma^2\)</span>, and hence by Markov the probability that <span class="math inline">\(Y &gt; k^2\sigma^2\)</span> is at most <span class="math inline">\(1/k^2\)</span>.
But clearly <span class="math inline">\((X-\mu)^2 \geq k^2\sigma^2\)</span> if and only if <span class="math inline">\(|X-\mu| \geq k\sigma\)</span>.</p>
</blockquote>
<p>One example of how to use Chebyshev’s inequality is the setting when <span class="math inline">\(X = X_1 + \cdots + X_n\)</span> where <span class="math inline">\(X_i\)</span>’s are <em>independent and identically distributed</em> (i.i.d for short) variables with values in <span class="math inline">\([0,1]\)</span> where each has expectation <span class="math inline">\(1/2\)</span>.
Since <span class="math inline">\(\mathbb{E}[X] = \sum_i \mathbb{E}[X_i] = n/2\)</span>, we would like to say that <span class="math inline">\(X\)</span> is very likely to be in, say, the interval <span class="math inline">\([0.499n,0.501n]\)</span>.
Using Markov’s inequality directly will not help us, since it will only tell us that <span class="math inline">\(X\)</span> is very likely to be at most <span class="math inline">\(100n\)</span> (which we already knew, since it always lies between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>).
However, since <span class="math inline">\(X_1,\ldots,X_n\)</span> are independent,
<span class="math display">\[
\mathrm{Var}[X_1+\cdots +X_n] = \mathrm{Var}[X_1]+\cdots + \mathrm{Var}[X_n]  \label{varianceeq}\;.
\]</span>
(We leave showing this to the reader as <a href="" class="ref">varianceex</a>.)</p>
<p>For every random variable <span class="math inline">\(X_i\)</span> in <span class="math inline">\([0,1]\)</span>, <span class="math inline">\(\mathrm{Var}[X_i] \leq 1\)</span> (if the variable is always in <span class="math inline">\([0,1]\)</span>, it can’t be more than <span class="math inline">\(1\)</span> away from its expectation), and hence <a href="" class="eqref">varianceeq</a> implies that <span class="math inline">\(\mathrm{Var}[X]\leq n\)</span> and hence <span class="math inline">\(\sigma[X] \leq \sqrt{n}\)</span>.
For large <span class="math inline">\(n\)</span>, <span class="math inline">\(\sqrt{n} \ll 0.001n\)</span>, and in particular if <span class="math inline">\(\sqrt{n} \leq 0.001n/k\)</span>, we can use Chebyshev’s inequality to bound the probability that <span class="math inline">\(X\)</span> is not in <span class="math inline">\([0.499n,0.501n]\)</span> by <span class="math inline">\(1/k^2\)</span>.</p>
</div>
<div id="the-chernoff-bound" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> The Chernoff bound<a href="mathematical-background.html#the-chernoff-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chebyshev’s inequality already shows a connection between independence and concentration, but in many cases we can hope for a quantitatively much stronger result.
If, as in the example above, <span class="math inline">\(X= X_1+\ldots+X_n\)</span> where the <span class="math inline">\(X_i\)</span>’s are bounded i.i.d random variables of mean <span class="math inline">\(1/2\)</span>, then as <span class="math inline">\(n\)</span> grows, the distribution of <span class="math inline">\(X\)</span> would be roughly the <em>normal</em> or <em>Gaussian</em> distribution<span class="math inline">\(-\)</span> that is, distributed according to the <em>bell curve</em> (see <a href="" class="ref">bellfig</a> and <a href="" class="ref">empiricalbellfig</a>).
This distribution has the property of being <em>very</em> concentrated in the sense that the probability of deviating <span class="math inline">\(k\)</span> standard deviations from the mean is not merely <span class="math inline">\(1/k^2\)</span> as is guaranteed by Chebyshev, but rather is roughly <span class="math inline">\(e^{-k^2}\)</span>.
Specifically, for a normal random variable <span class="math inline">\(X\)</span> of expectation <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, the probability that <span class="math inline">\(|X-\mu| \geq k\sigma\)</span> is at most <span class="math inline">\(2e^{-k^2/2}\)</span>.
That is, we have an <em>exponential decay</em> of the probability of deviation.</p>
<div class="float" id="empiricalbellfig">
<img src="../figure/sixsigma.jpg" class="margin" alt="In the normal distribution or the Bell curve, the probability of deviating k standard deviations from the expectation shrinks exponentially in k^2, and specifically with probability at least 1-2e^{-k^2/2}, a random variable X of expectation \mu and standard deviation \sigma satisfies \mu -k\sigma \leq X \leq \mu+k\sigma. This figure gives more precise bounds for k=1,2,3,4,5,6. (Image credit:Imran Baghirov)" />
<div class="figcaption">In the <em>normal distribution</em> or the Bell curve, the probability of deviating <span class="math inline">\(k\)</span> standard deviations from the expectation shrinks <em>exponentially</em> in <span class="math inline">\(k^2\)</span>, and specifically with probability at least <span class="math inline">\(1-2e^{-k^2/2}\)</span>, a random variable <span class="math inline">\(X\)</span> of expectation <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> satisfies <span class="math inline">\(\mu -k\sigma \leq X \leq \mu+k\sigma\)</span>. This figure gives more precise bounds for <span class="math inline">\(k=1,2,3,4,5,6\)</span>. (Image credit:Imran Baghirov)</div>
</div>
<p>The following extremely useful theorem shows that such exponential decay occurs every time we have a sum of independent and bounded variables. This theorem is known under many names in different communities, though it is mostly called the <a href="https://en.wikipedia.org/wiki/Chernoff_bound">Chernoff bound</a> in the computer science literature:</p>
<blockquote>
<h3 id="chernoffthm" class="theorem" title="Chernoff/Hoeffding bound"></h3>
<p>If <span class="math inline">\(X_1,\ldots,X_n\)</span> are i.i.d random variables such that <span class="math inline">\(X_i \in [0,1]\)</span> and <span class="math inline">\(\mathbb{E}[X_i]=p\)</span> for every <span class="math inline">\(i\)</span>,
then for every <span class="math inline">\(\epsilon &gt;0\)</span>
<span class="math display">\[
\Pr[ \left| \sum_{i=0}^{n-1} X_i - pn \right| &gt; \epsilon n ] \leq 2\cdot e^{-2\epsilon^2 n} .
\]</span></p>
</blockquote>
<p>We omit the proof, which appears in many texts, and uses Markov’s inequality on i.i.d random variables <span class="math inline">\(Y_0,\ldots,Y_n\)</span> that are of the form <span class="math inline">\(Y_i = e^{\lambda X_i}\)</span> for some carefully chosen parameter <span class="math inline">\(\lambda\)</span>.
See <a href="" class="ref">chernoffstirlingex</a> for a proof of the simple (but highly useful and representative) case where each <span class="math inline">\(X_i\)</span> is <span class="math inline">\(\{0,1\}\)</span> valued and <span class="math inline">\(p=1/2\)</span>.
(See also <a href="" class="ref">poorchernoff</a> for a generalization.)</p>
</div>
</div>
<div id="exercises" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Exercises<a href="mathematical-background.html#exercises" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<h3 id="section-13" class="exercise"></h3>
<p>Prove that for every finite <span class="math inline">\(S,T\)</span>, there are <span class="math inline">\((|T|+1)^{|S|}\)</span> partial functions from <span class="math inline">\(S\)</span> to <span class="math inline">\(T\)</span>.</p>
</blockquote>
<div class="exercise" title="$O$-notation">
<p><span id="exr:ohnotationex" class="exercise"><strong>Exercise 1.1  </strong></span>For every pair of functions <span class="math inline">\(F,G\)</span> below, determine which of the following relations holds: <span class="math inline">\(F=O(G)\)</span>, <span class="math inline">\(F=\Omega(G)\)</span>, <span class="math inline">\(F=o(G)\)</span> or <span class="math inline">\(F=\omega(G)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(F(n)=n\)</span>, <span class="math inline">\(G(n)=100n\)</span>.</p></li>
<li><p><span class="math inline">\(F(n)=n\)</span>, <span class="math inline">\(G(n)=\sqrt{n}\)</span>.</p></li>
<li><p><span class="math inline">\(F(n)=n\log n\)</span>, <span class="math inline">\(G(n)=2^{(\log (n))^2}\)</span>.</p></li>
<li><p><span class="math inline">\(F(n)=\sqrt{n}\)</span>, <span class="math inline">\(G(n)=2^{\sqrt{\log n}}\)</span></p></li>
<li><p><span class="math inline">\(F(n) = \binom{n}{\ceil{0.2 n}}\)</span> , <span class="math inline">\(G(n) = 2^{0.1 n}\)</span> (where <span class="math inline">\(\binom{n}{k}\)</span> is the number of <span class="math inline">\(k\)</span>-sized subsets of a set of size <span class="math inline">\(n\)</span>) and <span class="math inline">\(g(n) = 2^{0.1 n}\)</span>. See footnote for hint.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p></li>
</ol>
</div>
<blockquote>
<h3 id="section-14" class="exercise"></h3>
<p>Give an example of a pair of functions <span class="math inline">\(F,G:\N \rightarrow \N\)</span> such that neither <span class="math inline">\(F=O(G)\)</span> nor <span class="math inline">\(G=O(F)\)</span> holds.</p>
</blockquote>
<div class="exercise" title="Properties of expectation and variance">
<p><span id="exr:propexpecvariance" class="exercise"><strong>Exercise 1.2  </strong></span>In the following exercise <span class="math inline">\(X,Y\)</span> denote random variables over some sample space <span class="math inline">\(S\)</span>. You can assume that the probability on <span class="math inline">\(S\)</span> is the uniform
distribution— every point <span class="math inline">\(s\)</span> is output with probability <span class="math inline">\(1/|S|\)</span>. Thus <span class="math inline">\({\mathbb{E}}[X]= (1/|S|)\sum_{s\in S}X(s)\)</span>. We define the variance and standard deviation of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as above (e.g., <span class="math inline">\(Var[X] = {\mathbb{E}}[(X-{\mathbb{E}}[X])^2 ]\)</span> and the standard deviation is the square root of
the variance). You can reuse your answers to prior questions in the later ones.</p>
<ol style="list-style-type: decimal">
<li><p>Prove that <span class="math inline">\(Var[X]\)</span> is always non-negative.</p></li>
<li><p>Prove that <span class="math inline">\(Var[X] = {\mathbb{E}}[X^2] - {\mathbb{E}}[X]^2\)</span>.</p></li>
<li><p>Prove that always <span class="math inline">\({\mathbb{E}}[X^2] \geq {\mathbb{E}}[X]^2\)</span>.</p></li>
<li><p>Give an example for a random variable <span class="math inline">\(X\)</span> such that <span class="math inline">\({\mathbb{E}}[X^2] &gt; {\mathbb{E}}[X]^2\)</span>.</p></li>
<li><p>Give an example for a random variable <span class="math inline">\(X\)</span> such that its standard deviation is <em>not equal</em> to <span class="math inline">\({\mathbb{E}}[ | X - {\mathbb{E}}[X] | ]\)</span>.</p></li>
<li><p>Give an example for a random variable <span class="math inline">\(X\)</span> such that its standard deviation is <em>equal to</em> to <span class="math inline">\({\mathbb{E}}[ | X - {\mathbb{E}}[X] | ]\)</span>.</p></li>
<li><p>Give an example for two random variables <span class="math inline">\(X,Y\)</span> such that <span class="math inline">\({\mathbb{E}}[XY] = {\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span>.</p></li>
<li><p>Give an example for two random variables <span class="math inline">\(X,Y\)</span> such that <span class="math inline">\({\mathbb{E}}[XY] \neq {\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span>.</p></li>
<li><p>Prove that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables (i.e., for every <span class="math inline">\(x,y\)</span>, <span class="math inline">\(\Pr[X=x \wedge Y=y]=\Pr[X=x]\Pr[Y=y]\)</span>) then <span class="math inline">\({\mathbb{E}}[XY]={\mathbb{E}}[X]{\mathbb{E}}[Y]\)</span> and <span class="math inline">\(Var[X+Y]=Var[X]+Var[Y]\)</span>.</p></li>
</ol>
</div>
<div class="exercise" title="Random hash function">
<p><span id="exr:randomfunction" class="exercise"><strong>Exercise 1.3  </strong></span>Suppose that <span class="math inline">\(H\)</span> is chosen to be a random function mapping the numbers <span class="math inline">\(\{1,\ldots,n\}\)</span> to the numbers <span class="math inline">\(\{1,..,m \}\)</span>. That is, for every <span class="math inline">\(i\in \{1,\ldots,n\}\)</span>, <span class="math inline">\(H(i)\)</span> is chosen to be a random number in <span class="math inline">\(\{ 1,\ldots, m\}\)</span> and that choice is done independently for every <span class="math inline">\(i\)</span>. For every <span class="math inline">\(i&lt;j \in \{1,\ldots,n\}\)</span>, define the random variable <span class="math inline">\(X_{i,j}\)</span> to equal <span class="math inline">\(1\)</span> if there was a <em>collision</em> between <span class="math inline">\(H(i)\)</span> and <span class="math inline">\(H(j)\)</span> in the sense that <span class="math inline">\(H(i)=H(j)\)</span> and to equal <span class="math inline">\(0\)</span> otherwise.</p>
<ol style="list-style-type: decimal">
<li><p>For every <span class="math inline">\(i&lt;j\)</span>, compute <span class="math inline">\({\mathbb{E}}[ X_{i,j} ]\)</span>.</p></li>
<li><p>Define <span class="math inline">\(Y = \sum_{i&lt;j} X_{i,j}\)</span> to be the total number of collisions.
Compute <span class="math inline">\({\mathbb{E}}[ Y ]\)</span> as a function of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>. In particular
your answer should imply that if <span class="math inline">\(m &lt; n^2/1000\)</span> then <span class="math inline">\({\mathbb{E}}[Y]&gt;1\)</span>
and hence in expectation there should be at least one collision and so
the function <span class="math inline">\(H\)</span> will not be one to one.</p></li>
<li><p>Prove that if <span class="math inline">\(m &gt; 1000\cdot n^2\)</span> then the probability that <span class="math inline">\(H\)</span> is one
to one is at least <span class="math inline">\(0.9\)</span>.</p></li>
<li><p>Give an example of a random variable <span class="math inline">\(Z\)</span> (unrelated to the function <span class="math inline">\(H\)</span>)
that is always equal to a non-negative integer, and such that
<span class="math inline">\({\mathbb{E}}[Z] \geq 1000\)</span> but <span class="math inline">\(\Pr[ Z &gt; 0] &lt; 0.001\)</span>.</p></li>
<li><p>Prove that if <span class="math inline">\(m &lt; n^2/1000\)</span> then the probability that <span class="math inline">\(H\)</span> is one to one
is at most <span class="math inline">\(0.1\)</span>.</p></li>
</ol>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="1.7">
<h2><span class="header-section-number">1.7</span> Exercises<a href="mathematical-background.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<blockquote>
<h3 id="section-15" class="exercise"></h3>
<p>Suppose that we toss three independent fair coins <span class="math inline">\(a,b,c \in \{0,1\}\)</span>. What is the probability that the XOR of <span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> is equal to <span class="math inline">\(1\)</span>? What is the probability that the AND of these three values is equal to <span class="math inline">\(1\)</span>? Are these two events independent?</p>
</blockquote>
<blockquote>
<h3 id="section-16" class="exercise"></h3>
<p>Give an example of random variables <span class="math inline">\(X,Y: \{0,1\}^3 \rightarrow \mathbb{R}\)</span> such that
<span class="math inline">\(\mathbb{E}[XY] \neq \mathbb{E}[X]\mathbb{E}[Y]\)</span>.</p>
</blockquote>
<blockquote>
<h3 id="noindnocorex" class="exercise"></h3>
<p>Give an example of random variables <span class="math inline">\(X,Y: \{0,1\}^3 \rightarrow \mathbb{R}\)</span> such that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>not</em> independent but <span class="math inline">\(\mathbb{E}[XY] =\mathbb{E}[X]\mathbb{E}[Y]\)</span>.</p>
</blockquote>
<blockquote>
<h3 id="expprodex" class="exercise" title="Product of expectations"></h3>
<p>Prove <a href="" class="ref">expprod</a></p>
</blockquote>
<blockquote>
<h3 id="indeplemex" class="exercise" title="Transformations preserve independence"></h3>
<p>Prove <a href="" class="ref">indeplem</a></p>
</blockquote>
<blockquote>
<h3 id="varianceex" class="exercise" title="Variance of independent random variables"></h3>
<p>Prove that if <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are independent random variables then <span class="math inline">\(\mathrm{Var}[X_0+\cdots+X_{n-1}]=\sum_{i=0}^{n-1} \mathrm{Var}[X_i]\)</span>.</p>
</blockquote>
<blockquote>
<h3 id="entropyex" class="exercise" title="Entropy (challenge)"></h3>
<p>Recall the definition of a distribution <span class="math inline">\(\mu\)</span> over some finite set <span class="math inline">\(S\)</span>.
Shannon defined the <em>entropy</em> of a distribution <span class="math inline">\(\mu\)</span>, denoted by <span class="math inline">\(H(\mu)\)</span>, to be <span class="math inline">\(\sum_{x\in S} \mu(x)\log(1/\mu(x))\)</span>.
The idea is that if <span class="math inline">\(\mu\)</span> is a distribution of entropy <span class="math inline">\(k\)</span>, then encoding members of <span class="math inline">\(\mu\)</span> will require <span class="math inline">\(k\)</span> bits, in an amortized sense.
In this exercise we justify this definition. Let <span class="math inline">\(\mu\)</span> be such that <span class="math inline">\(H(\mu)=k\)</span>.<br />
1. Prove that for every one to one function <span class="math inline">\(F:S \rightarrow \{0,1\}^*\)</span>, <span class="math inline">\(\mathbb{E}_{x \sim \mu} |F(x)| \geq k\)</span>.<br />
2. Prove that for every <span class="math inline">\(\epsilon\)</span>, there is some <span class="math inline">\(n\)</span> and a one-to-one function <span class="math inline">\(F:S^n \rightarrow \{0,1\}^*\)</span>, such that <span class="math inline">\(\mathbb{E}_{x\sim \mu^n} |F(x)| \leq n(k+\epsilon)\)</span>,
where <span class="math inline">\(x \sim \mu\)</span> denotes the experiments of choosing <span class="math inline">\(x_0,\ldots,x_{n-1}\)</span> each independently from <span class="math inline">\(S\)</span> using the distribution <span class="math inline">\(\mu\)</span>.</p>
</blockquote>
<div class="exercise" title="Entropy approximation to binomial">
<p><span id="exr:entropybinomex" class="exercise"><strong>Exercise 1.4  </strong></span>Let <span class="math inline">\(H(p) = p \log(1/p)+(1-p)\log(1/(1-p))\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Prove that for every <span class="math inline">\(p \in (0,1)\)</span> and <span class="math inline">\(\epsilon&gt;0\)</span>, if <span class="math inline">\(n\)</span> is large enough then<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
<span class="math display">\[
2^{(H(p)-\epsilon)n }\binom{n}{pn} \leq 2^{(H(p)+\epsilon)n}
\]</span>
where <span class="math inline">\(\binom{n}{k}\)</span> is the binomial coefficient <span class="math inline">\(\tfrac{n!}{k!(n-k)!}\)</span> which is equal to the number of <span class="math inline">\(k\)</span>-size subsets of <span class="math inline">\(\{0,\ldots,n-1\}\)</span>.</p>
</div>
<div class="exercise" title="Chernoff using Stirling">
<p><span id="exr:chernoffstirlingex" class="exercise"><strong>Exercise 1.5  </strong></span></p>
<ol style="list-style-type: decimal">
<li><p>Prove that <span class="math inline">\(\Pr_{x\sim \{0,1\}^n}[ \sum x_i = k ] = \binom{n}{k}2^{-n}\)</span>.<br />
</p></li>
<li><p>Use this and <a href="" class="ref">entropybinomex</a> to prove (an approximate version of) the Chernoff bound for the case that <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are i.i.d. random variables over <span class="math inline">\(\{0,1\}\)</span> each equaling <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> with probability <span class="math inline">\(1/2\)</span>. That is, prove that for every <span class="math inline">\(\epsilon&gt;0\)</span>, and <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> as above, <span class="math inline">\(\Pr[ |\sum_{i=0}^{n-1} - \tfrac{n/2}| &gt; \epsilon n] &lt; 2^{0.1 \cdot \epsilon^2 n}\)</span>.</p></li>
</ol>
</div>
<div class="exercise" title="Poor man&#39;s Chernoff">
<p><span id="exr:poorchernoff" class="exercise"><strong>Exercise 1.6  </strong></span><a href="" class="ref">chernoffstirlingex</a> establishes the Chernoff bound for the case that <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> are i.i.d variables over <span class="math inline">\(\{0,1\}\)</span> with expectation <span class="math inline">\(1/2\)</span>.
In this exercise we use a slightly different method (bounding the <em>moments</em> of the random variables) to establish a version of Chernoff
where the random variables range over <span class="math inline">\([0,1]\)</span> and their expectation is some number <span class="math inline">\(p \in [0,1]\)</span> that may be different than <span class="math inline">\(1/2\)</span>.
Let <span class="math inline">\(X_0,\ldots,X_{n-1}\)</span> be i.i.d random variables with <span class="math inline">\(\mathbb{E}X_i = p\)</span> and <span class="math inline">\(\Pr [ 0 \leq X_i \leq 1 ]=1\)</span>.
Define <span class="math inline">\(Y_i = X_i - p\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Prove that for every <span class="math inline">\(j_0,\ldots,j_{n-1} \in \N\)</span>, if there exists one <span class="math inline">\(i\)</span> such that <span class="math inline">\(j_i\)</span> is odd then <span class="math inline">\(\mathbb{E}[\prod_{i=0}^{n-1} Y_i^{j_i}] = 0\)</span>.<br />
</p></li>
<li><p>Prove that for every <span class="math inline">\(k\)</span>, <span class="math inline">\(\mathbb{E}[ (\sum_{i=0}^{n-1} Y_i)^k ] \leq (10kn)^{k/2}\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a><br />
</p></li>
<li><p>Prove that for every <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math inline">\(\Pr[ |\sum_i Y_i| \geq \epsilon n ] \geq 2^{-\epsilon^2 n / (10000\log 1/\epsilon)}\)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p></li>
</ol>
</div>
<div class="exercise" title="Lower bound for distinguishing coins">
<p><span id="exr:lowerboundcoins" class="exercise"><strong>Exercise 1.7  </strong></span>The Chernoff bound can be used to show that if you were given a coin of bias at least <span class="math inline">\(\epsilon\)</span>, you should only need <span class="math inline">\(O(1/\epsilon^2)\)</span> samples to be able to reject the “null hypothesis” that the coin is completely unbiased with extremely high confidence. In the following somewhat more challenging question, we try to show a converse to this, proving that distinguishing between a fair every coin and a coin that outputs “heads” with probability <span class="math inline">\(1/2 + \epsilon\)</span> requires at least <span class="math inline">\(\Omega(1/\epsilon^2)\)</span> samples.</p>
<p>Let <span class="math inline">\(P\)</span> be the uniform distribution over <span class="math inline">\({\{0,1\}}^n\)</span> and <span class="math inline">\(Q\)</span> be the <span class="math inline">\(1/2+\epsilon\)</span>-biased distribution corresponding to tossing <span class="math inline">\(n\)</span> coins in which each one has a probability of <span class="math inline">\(1/2+\epsilon\)</span> of equaling <span class="math inline">\(1\)</span> and probability <span class="math inline">\(1/2-\epsilon\)</span> of equaling <span class="math inline">\(0\)</span>. Namely the probability of
<span class="math inline">\(x\in{\{0,1\}}^n\)</span> according to <span class="math inline">\(Q\)</span> is equal to <span class="math inline">\(\prod_{i=1}^n (1/2 - \epsilon + 2\epsilon x_i)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Prove that for every threshold <span class="math inline">\(\theta\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(n\)</span>, if <span class="math inline">\(n &lt; 1/(100\epsilon)^2\)</span> then the probabilities that <span class="math inline">\(\sum x_i \leq \theta\)</span> under <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively differ by at most <span class="math inline">\(0.1\)</span>. Therefore, one cannot use the test whether the number of heads is above or below some threshold to reliably distinguish between these two possibilities unless the number of samples <span class="math inline">\(n\)</span> of the coins is at least some constant times <span class="math inline">\(1/\epsilon^2\)</span>.</p></li>
<li><p>Prove that for <em>every</em> function <span class="math inline">\(F\)</span> mapping <span class="math inline">\({\{0,1\}}^n\)</span> to <span class="math inline">\({\{0,1\}}\)</span>, if <span class="math inline">\(n &lt; 1/(100\epsilon)^2\)</span> then the probabilities that <span class="math inline">\(F(x)=1\)</span> under <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> respectively differ by at most <span class="math inline">\(0.1\)</span>. Therefore, if the number of samples is smaller than a constant times <span class="math inline">\(1/\epsilon^2\)</span> then there is simply <em>no test</em> that can reliably distinguish between these two possibilities.</p></li>
</ol>
</div>
<blockquote>
<h3 id="coindistex" class="exercise" title="Simulating distributions using coins"></h3>
<p>Our model for probability involves tossing <span class="math inline">\(n\)</span> coins, but sometimes algorithms require sampling from other distributions, such as selecting a uniform number in <span class="math inline">\(\{0,\ldots,M-1\}\)</span> for some <span class="math inline">\(M\)</span>.
Fortunately, we can simulate this with an exponentially small probability of error: prove that for every <span class="math inline">\(M\)</span>, if <span class="math inline">\(n&gt;k\lceil \log M \rceil\)</span>, then there is a function <span class="math inline">\(F:\{0,1\}^n \rightarrow \{0,\ldots,M-1\} \cup \{ \bot \}\)</span> such that <strong>(1)</strong> The probability that <span class="math inline">\(F(x)=\bot\)</span> is at most <span class="math inline">\(2^{-k}\)</span> and <strong>(2)</strong> the distribution of <span class="math inline">\(F(x)\)</span> conditioned on <span class="math inline">\(F(x) \neq \bot\)</span> is equal to the uniform distribution over <span class="math inline">\(\{0,\ldots,M-1\}\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
</blockquote>
<div class="exercise" title="Sampling">
<p><span id="exr:samplingex" class="exercise"><strong>Exercise 1.8  </strong></span>Suppose that a country has 300,000,000 citizens, 52 percent of which prefer the color “green” and 48 percent of which prefer the color “orange”. Suppose we sample <span class="math inline">\(n\)</span> random citizens and ask them their favorite color (assume they will answer truthfully). What is the smallest value <span class="math inline">\(n\)</span> among the following choices so that the probability that the majority of the sample answers “green” is at most <span class="math inline">\(0.05\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li><p>1,000</p></li>
<li><p>10,000</p></li>
<li><p>100,000</p></li>
<li><p>1,000,000</p></li>
</ol>
</div>
<blockquote>
<h3 id="exid" class="exercise"></h3>
<p>Would the answer to <a href="" class="ref">samplingex</a> change if the country had 300,000,000,000 citizens?</p>
</blockquote>
<div class="exercise" title="Sampling (2)">
<p><span id="exr:exidtwo" class="exercise"><strong>Exercise 1.9  </strong></span>Under the same assumptions as <a href="" class="ref">samplingex</a>, what is the smallest value <span class="math inline">\(n\)</span> among the following choices so that the probability that the majority of the sample answers “green” is at most <span class="math inline">\(2^{-100}\)</span>?</p>
<ol style="list-style-type: lower-alpha">
<li><p>1,000</p></li>
<li><p>10,000</p></li>
<li><p>100,000</p></li>
<li><p>1,000,000</p></li>
<li><p>It is impossible to get such low probability since there are fewer than <span class="math inline">\(2^{100}\)</span> citizens.</p></li>
</ol>
</div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Harvard’s <a href="http://projects.iq.harvard.edu/stat110/home">STAT 110</a> class (whose lectures are available on <a href="http://projects.iq.harvard.edu/stat110/youtube">youtube</a> ) is a highly recommended introduction to probability. See also these <a href="http://www.boazbarak.org/cs121/LLM_probability.pdf">lecture notes</a> from MIT’s “Mathematics for Computer Science” course ,as well as notes 12-17 of Berkeley’s <a href="https://www.eecs70.org/">CS 70</a>.<a href="mathematical-background.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>one way to do this is to use <a href="https://goo.gl/cqEmS2">Stirling’s approximation for the factorial function.</a>.<a href="mathematical-background.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>While you don’t need this to solve this exercise, this is the function that maps <span class="math inline">\(p\)</span> to the entropy (as defined in <a href="" class="ref">entropyex</a>) of the <span class="math inline">\(p\)</span>-biased coin distribution over <span class="math inline">\(\{0,1\}\)</span>, which is the function <span class="math inline">\(\mu:\{0,1\}\rightarrow [0,1]\)</span> s.y. <span class="math inline">\(\mu(0)=1-p\)</span> and <span class="math inline">\(\mu(1)=p\)</span>.<a href="mathematical-background.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><strong>Hint:</strong> Use Stirling’s formula for approximating the factorial function.<a href="mathematical-background.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p><strong>Hint:</strong> Bound the number of tuples <span class="math inline">\(j_0,\ldots,j_{n-1}\)</span> such that every <span class="math inline">\(j_i\)</span> is even and <span class="math inline">\(\sum j_i = k\)</span> using the Binomial coefficient and the fact that in any such tuple there are at most <span class="math inline">\(k/2\)</span> distinct indices.<a href="mathematical-background.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><strong>Hint:</strong> Set <span class="math inline">\(k=2\lceil \epsilon^2 n /1000 \rceil\)</span> and then show that if the event <span class="math inline">\(|\sum Y_i | \geq \epsilon n\)</span> happens then the random variable <span class="math inline">\((\sum Y_i)^k\)</span> is a factor of <span class="math inline">\(\epsilon^{-k}\)</span> larger than its expectation.<a href="mathematical-background.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p><strong>Hint:</strong> Think of <span class="math inline">\(x\in \{0,1\}^n\)</span> as choosing <span class="math inline">\(k\)</span> numbers <span class="math inline">\(y_1,\ldots,y_k \in \{0,\ldots, 2^{\lceil \log M \rceil}-1 \}\)</span>. Output the first such number that is in <span class="math inline">\(\{0,\ldots,M-1\}\)</span>. <a href="mathematical-background.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="introduction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/00-mathematical-background.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
